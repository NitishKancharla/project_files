{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9893aa1e-3137-4d12-aefb-4ca7587b3c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chardet in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (5.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2dd1db4-5271-4a8b-9c74-c95a8535603c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/gemma-3-27b-it\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=\"AIzaSyDS0SXbtLKaawt2IdjTezO8HzsaSoM6RJM\")\n",
    "for m in genai.list_models():\n",
    "    if 'generateContent' in m.supported_generation_methods:\n",
    "        print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b334cf6b-191f-4853-a194-31c5f40ecbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytesseract\n",
      "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from pytesseract) (24.2)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from pytesseract) (11.1.0)\n",
      "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: pytesseract\n",
      "Successfully installed pytesseract-0.3.13\n",
      "Requirement already satisfied: google-generativeai in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (0.8.4)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-generativeai) (2.24.2)\n",
      "Requirement already satisfied: google-api-python-client in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-generativeai) (2.165.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-generativeai) (2.38.0)\n",
      "Requirement already satisfied: protobuf in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-generativeai) (5.29.3)\n",
      "Requirement already satisfied: pydantic in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-generativeai) (2.10.6)\n",
      "Requirement already satisfied: tqdm in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-generativeai) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-api-core->google-generativeai) (1.69.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from pydantic->google-generativeai) (2.27.2)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n",
      "Requirement already satisfied: PyPDF2 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (3.0.1)\n",
      "Requirement already satisfied: python-docx in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (1.1.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from python-docx) (5.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from python-docx) (4.12.2)\n",
      "Requirement already satisfied: spacy in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from spacy) (77.0.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: wrapt in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytesseract\n",
    "!pip install google-generativeai\n",
    "!pip install PyPDF2\n",
    "!pip install python-docx\n",
    "!pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93179307-bf05-4b92-add7-c078a4182c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading spaCy model...\n",
      "Collecting en-core-web-lg==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "* Running on local URL:  http://127.0.0.1:7871\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7871/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini validation error: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import spacy\n",
    "import re\n",
    "import json\n",
    "import pytesseract\n",
    "import google.generativeai as genai\n",
    "import PyPDF2\n",
    "import docx\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Configure Gemini API\n",
    "genai.configure(api_key=\"AIzaSyBQYHE915iZLgTibkof6Un5sJmyCFK_IO0\")\n",
    "\n",
    "# Load spaCy model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "except OSError:\n",
    "    print(\"Downloading spaCy model...\")\n",
    "    spacy.cli.download(\"en_core_web_lg\")\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Predefined Job Descriptions\n",
    "JOB_DESCRIPTIONS = {\n",
    "    \"Data Scientist\": {\n",
    "        \"required_skills\": [\n",
    "            \"Python\", \"Machine Learning\", \"Data Analysis\", \n",
    "            \"SQL\", \"Pandas\", \"NumPy\", \"TensorFlow\", \"scikit-learn\"\n",
    "        ],\n",
    "        \"description\": \"Analyze complex data sets, develop machine learning models, and provide actionable insights.\"\n",
    "    },\n",
    "    \"Software Engineer\": {\n",
    "        \"required_skills\": [\n",
    "            \"Python\", \"JavaScript\", \"React\", \"Node.js\", \n",
    "            \"SQL\", \"Git\", \"Docker\", \"Cloud Computing\"\n",
    "        ],\n",
    "        \"description\": \"Develop and maintain software applications, collaborate with cross-functional teams, and implement new features.\"\n",
    "    },\n",
    "    \"Frontend Developer\": {\n",
    "        \"required_skills\": [\n",
    "            \"JavaScript\", \"React\", \"HTML\", \"CSS\", \n",
    "            \"TypeScript\", \"Vue.js\", \"Angular\", \"Responsive Design\"\n",
    "        ],\n",
    "        \"description\": \"Create responsive and interactive web applications with a focus on user experience.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def extract_text_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Extract text from various file types\n",
    "    \"\"\"\n",
    "    file_extension = file_path.split('.')[-1].lower()\n",
    "    \n",
    "    try:\n",
    "        # PDF handling\n",
    "        if file_extension == 'pdf':\n",
    "            with open(file_path, 'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                text = ''\n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text() or ''\n",
    "                return text\n",
    "        \n",
    "        # DOCX handling\n",
    "        elif file_extension == 'docx':\n",
    "            doc = docx.Document(file_path)\n",
    "            return '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
    "        \n",
    "        # Text file handling\n",
    "        else:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                return file.read()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_personal_info(doc):\n",
    "    \"\"\"\n",
    "    Extract personal information using spaCy\n",
    "    \"\"\"\n",
    "    personal_info = {\n",
    "        \"name\": \"\",\n",
    "        \"email\": \"\",\n",
    "        \"phone\": \"\",\n",
    "        \"address\": \"\",\n",
    "        \"social\": []\n",
    "    }\n",
    "    \n",
    "    # Extract email\n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "    emails = re.findall(email_pattern, doc.text)\n",
    "    personal_info[\"email\"] = emails[0] if emails else \"\"\n",
    "    \n",
    "    # Extract phone number\n",
    "    phone_pattern = r'\\b(?:\\+\\d{1,2}\\s?)?(?:\\(\\d{3}\\)|\\d{3})[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b'\n",
    "    phones = re.findall(phone_pattern, doc.text)\n",
    "    personal_info[\"phone\"] = phones[0] if phones else \"\"\n",
    "    \n",
    "    # Extract name (using the first person entity)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\" and len(ent.text.split()) > 1:\n",
    "            personal_info[\"name\"] = ent.text\n",
    "            break\n",
    "    \n",
    "    # Extract social links\n",
    "    social_pattern = r'(https?://\\S+(?:linkedin\\.com|github\\.com|twitter\\.com)\\S+)'\n",
    "    personal_info[\"social\"] = re.findall(social_pattern, doc.text)\n",
    "    \n",
    "    return personal_info\n",
    "\n",
    "def extract_skills(doc):\n",
    "    \"\"\"\n",
    "    Extract skills using spaCy and regex\n",
    "    \"\"\"\n",
    "    # Common skill keywords\n",
    "    skill_keywords = [\n",
    "        \"python\", \"javascript\", \"java\", \"c++\", \"sql\", \"react\", \n",
    "        \"machine learning\", \"data analysis\", \"docker\", \"git\",\n",
    "        \"tensorflow\", \"pandas\", \"numpy\", \"scikit-learn\",\n",
    "        \"html\", \"css\", \"node.js\", \"cloud computing\"\n",
    "    ]\n",
    "    \n",
    "    skills = []\n",
    "    for token in doc:\n",
    "        # Check for skill keywords\n",
    "        if token.text.lower() in skill_keywords:\n",
    "            skills.append({\n",
    "                \"name\": token.text.capitalize(),\n",
    "                \"type\": \"technical\",\n",
    "                \"experience\": 0  # To be refined later\n",
    "            })\n",
    "    \n",
    "    return skills\n",
    "\n",
    "def extract_education(doc):\n",
    "    \"\"\"\n",
    "    Extract education information\n",
    "    \"\"\"\n",
    "    education = []\n",
    "    \n",
    "    # Look for education-related entities and patterns\n",
    "    edu_patterns = [\n",
    "        r'(Bachelor|Master|PhD|Doctorate)[\\s]+(of|in)?[\\s]*([\\w\\s]+)',\n",
    "        r'(B\\.?A\\.?|B\\.?S\\.?|M\\.?S\\.?|M\\.?A\\.?|Ph\\.?D\\.?)\\s+(in)?[\\s]*([\\w\\s]+)'\n",
    "    ]\n",
    "    \n",
    "    for pattern in edu_patterns:\n",
    "        matches = re.findall(pattern, doc.text, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            degree = match[0]\n",
    "            field = match[2] if len(match) > 2 else \"\"\n",
    "            education.append({\n",
    "                \"degree\": degree,\n",
    "                \"institution\": \"\",  # To be refined\n",
    "                \"dates\": \"\"  # To be refined\n",
    "            })\n",
    "    \n",
    "    return education\n",
    "\n",
    "def extract_experience(doc):\n",
    "    \"\"\"\n",
    "    Extract work experience details\n",
    "    \"\"\"\n",
    "    experience = []\n",
    "    \n",
    "    # Patterns for job titles and companies\n",
    "    job_patterns = [\n",
    "        r'([\\w\\s]+)\\s+at\\s+([\\w\\s]+)',\n",
    "        r'([\\w\\s]+)\\s+in\\s+([\\w\\s]+)\\s+company'\n",
    "    ]\n",
    "    \n",
    "    for pattern in job_patterns:\n",
    "        matches = re.findall(pattern, doc.text, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            experience.append({\n",
    "                \"title\": match[0],\n",
    "                \"company\": match[1],\n",
    "                \"dates\": \"\",  # To be refined\n",
    "                \"location\": \"\"  # To be refined\n",
    "            })\n",
    "    \n",
    "    return experience\n",
    "\n",
    "def validate_with_gemini(extracted_data, resume_text):\n",
    "    \"\"\"\n",
    "    Validate and refine extracted data using Gemini\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel('gemini-pro')\n",
    "    \n",
    "    prompt = f\"\"\"Carefully review and validate the extracted resume information. \n",
    "    Provide a refined, complete JSON representation of the resume details.\n",
    "\n",
    "    Extracted Data:\n",
    "    {json.dumps(extracted_data)}\n",
    "\n",
    "    Original Resume Text:\n",
    "    {resume_text}\n",
    "\n",
    "    Requirements:\n",
    "    - Validate and correct extracted information\n",
    "    - Fill in missing details\n",
    "    - Ensure JSON is complete and accurate\n",
    "    - Add reasonable estimates for missing fields\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        # Clean and parse the response\n",
    "        json_text = response.text.strip('```json').strip('```').strip()\n",
    "        validated_data = json.loads(json_text)\n",
    "        return validated_data\n",
    "    except Exception as e:\n",
    "        print(f\"Gemini validation error: {e}\")\n",
    "        return extracted_data\n",
    "\n",
    "def process_resume(resume_file, job_role):\n",
    "    \"\"\"\n",
    "    Main resume processing function\n",
    "    \"\"\"\n",
    "    # Extract text from file\n",
    "    resume_text = extract_text_from_file(resume_file.name)\n",
    "    \n",
    "    # Process with spaCy\n",
    "    doc = nlp(resume_text)\n",
    "    \n",
    "    # Extract initial information\n",
    "    extracted_data = {\n",
    "        \"personal\": extract_personal_info(doc),\n",
    "        \"skills\": extract_skills(doc),\n",
    "        \"education\": extract_education(doc),\n",
    "        \"experience\": extract_experience(doc),\n",
    "        \"overall_experience\": 0,\n",
    "        \"salary\": 0.0\n",
    "    }\n",
    "    \n",
    "    # Validate and refine with Gemini\n",
    "    refined_data = validate_with_gemini(extracted_data, resume_text)\n",
    "    \n",
    "    # Match job description\n",
    "    job_match_result = match_job_description(refined_data, job_role)\n",
    "    \n",
    "    # Combine results\n",
    "    final_result = {\n",
    "        \"resume_details\": refined_data,\n",
    "        \"job_match\": job_match_result\n",
    "    }\n",
    "    \n",
    "    return json.dumps(final_result, indent=2)\n",
    "\n",
    "def match_job_description(resume_data, job_role):\n",
    "    \"\"\"\n",
    "    Match resume skills with job description\n",
    "    \"\"\"\n",
    "    job_description = JOB_DESCRIPTIONS.get(job_role, {})\n",
    "    required_skills = job_description.get('required_skills', [])\n",
    "    \n",
    "    # Extract skill names\n",
    "    resume_skills = [\n",
    "        skill.get('name', '').lower() \n",
    "        for skill in resume_data.get('skills', [])\n",
    "    ]\n",
    "    \n",
    "    # Calculate matching skills\n",
    "    matched_skills = [\n",
    "        skill for skill in required_skills \n",
    "        if skill.lower() in resume_skills\n",
    "    ]\n",
    "    unmatched_skills = list(set(required_skills) - set(matched_skills))\n",
    "    \n",
    "    # Calculate match score\n",
    "    match_percentage = (len(matched_skills) / len(required_skills)) * 100 if required_skills else 0\n",
    "    \n",
    "    return {\n",
    "        \"match_score\": round(match_percentage, 2),\n",
    "        \"matched_skills\": matched_skills,\n",
    "        \"unmatched_skills\": unmatched_skills,\n",
    "        \"job_description\": job_description.get('description', '')\n",
    "    }\n",
    "\n",
    "# Gradio Interface\n",
    "def create_gradio_interface():\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"# Advanced Resume Extraction\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            resume_input = gr.File(label=\"Upload Resume\")\n",
    "            job_role_dropdown = gr.Dropdown(\n",
    "                choices=list(JOB_DESCRIPTIONS.keys()), \n",
    "                label=\"Select Job Role\"\n",
    "            )\n",
    "        \n",
    "        submit_btn = gr.Button(\"Process Resume\")\n",
    "        output = gr.TextArea(label=\"Results\")\n",
    "        \n",
    "        submit_btn.click(\n",
    "            fn=process_resume, \n",
    "            inputs=[resume_input, job_role_dropdown], \n",
    "            outputs=output\n",
    "        )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "# Launch the app\n",
    "if __name__ == \"__main__\":\n",
    "    demo = create_gradio_interface()\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d03b0874-e4b3-4957-9d8d-bafdfdefd49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Models: ['models/gemini-1.0-pro-vision-latest', 'models/gemini-pro-vision', 'models/gemini-1.5-pro-latest', 'models/gemini-1.5-pro-001', 'models/gemini-1.5-pro-002', 'models/gemini-1.5-pro', 'models/gemini-1.5-flash-latest', 'models/gemini-1.5-flash-001', 'models/gemini-1.5-flash-001-tuning', 'models/gemini-1.5-flash', 'models/gemini-1.5-flash-002', 'models/gemini-1.5-flash-8b', 'models/gemini-1.5-flash-8b-001', 'models/gemini-1.5-flash-8b-latest', 'models/gemini-1.5-flash-8b-exp-0827', 'models/gemini-1.5-flash-8b-exp-0924', 'models/gemini-2.0-flash-exp', 'models/gemini-2.0-flash', 'models/gemini-2.0-flash-001', 'models/gemini-2.0-flash-exp-image-generation', 'models/gemini-2.0-flash-lite-001', 'models/gemini-2.0-flash-lite', 'models/gemini-2.0-flash-lite-preview-02-05', 'models/gemini-2.0-flash-lite-preview', 'models/gemini-2.0-pro-exp', 'models/gemini-2.0-pro-exp-02-05', 'models/gemini-exp-1206', 'models/gemini-2.0-flash-thinking-exp-01-21', 'models/gemini-2.0-flash-thinking-exp', 'models/gemini-2.0-flash-thinking-exp-1219', 'models/learnlm-1.5-pro-experimental', 'models/gemma-3-27b-it']\n",
      "* Running on local URL:  http://127.0.0.1:7875\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7875/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI validation error: Extra data: line 106 column 1 (char 4456)\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import spacy\n",
    "import re\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "import PyPDF2\n",
    "import docx\n",
    "\n",
    "# Configure Gemini API\n",
    "genai.configure(api_key=\"AIzaSyBQYHE915iZLgTibkof6Un5sJmyCFK_IO0\")\n",
    "\n",
    "# Verify and list available models\n",
    "def get_available_models():\n",
    "    \"\"\"\n",
    "    Retrieve and print available generative models\n",
    "    \"\"\"\n",
    "    try:\n",
    "        models = [m.name for m in genai.list_models() \n",
    "                  if 'generateContent' in m.supported_generation_methods]\n",
    "        print(\"Available Models:\", models)\n",
    "        return models\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing models: {e}\")\n",
    "        return []\n",
    "\n",
    "# Load spaCy model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "except OSError:\n",
    "    print(\"Downloading spaCy model...\")\n",
    "    spacy.cli.download(\"en_core_web_lg\")\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Predefined Job Descriptions\n",
    "JOB_DESCRIPTIONS = {\n",
    "    \"Data Scientist\": {\n",
    "        \"required_skills\": [\n",
    "            \"Python\", \"Machine Learning\", \"Data Analysis\", \n",
    "            \"SQL\", \"Pandas\", \"NumPy\", \"TensorFlow\", \"scikit-learn\"\n",
    "        ],\n",
    "        \"description\": \"Analyze complex data sets, develop machine learning models, and provide actionable insights.\"\n",
    "    },\n",
    "    \"Software Engineer\": {\n",
    "        \"required_skills\": [\n",
    "            \"Python\", \"JavaScript\", \"React\", \"Node.js\", \n",
    "            \"SQL\", \"Git\", \"Docker\", \"Cloud Computing\"\n",
    "        ],\n",
    "        \"description\": \"Develop and maintain software applications, collaborate with cross-functional teams, and implement new features.\"\n",
    "    },\n",
    "    \"Frontend Developer\": {\n",
    "        \"required_skills\": [\n",
    "            \"JavaScript\", \"React\", \"HTML\", \"CSS\", \n",
    "            \"TypeScript\", \"Vue.js\", \"Angular\", \"Responsive Design\"\n",
    "        ],\n",
    "        \"description\": \"Create responsive and interactive web applications with a focus on user experience.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def extract_text_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Extract text from various file types\n",
    "    \"\"\"\n",
    "    file_extension = file_path.split('.')[-1].lower()\n",
    "    \n",
    "    try:\n",
    "        # PDF handling\n",
    "        if file_extension == 'pdf':\n",
    "            with open(file_path, 'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                text = ''\n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text() or ''\n",
    "                return text\n",
    "        \n",
    "        # DOCX handling\n",
    "        elif file_extension == 'docx':\n",
    "            doc = docx.Document(file_path)\n",
    "            return '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
    "        \n",
    "        # Text file handling\n",
    "        else:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                return file.read()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_personal_info(doc):\n",
    "    \"\"\"\n",
    "    Extract personal information using spaCy\n",
    "    \"\"\"\n",
    "    personal_info = {\n",
    "        \"name\": \"\",\n",
    "        \"email\": \"\",\n",
    "        \"phone\": \"\",\n",
    "        \"address\": \"\",\n",
    "        \"social\": []\n",
    "    }\n",
    "    \n",
    "    # Extract email\n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "    emails = re.findall(email_pattern, doc.text)\n",
    "    personal_info[\"email\"] = emails[0] if emails else \"\"\n",
    "    \n",
    "    # Extract phone number\n",
    "    phone_pattern = r'\\b(?:\\+\\d{1,2}\\s?)?(?:\\(\\d{3}\\)|\\d{3})[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b'\n",
    "    phones = re.findall(phone_pattern, doc.text)\n",
    "    personal_info[\"phone\"] = phones[0] if phones else \"\"\n",
    "    \n",
    "    # Extract name (using the first person entity)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\" and len(ent.text.split()) > 1:\n",
    "            personal_info[\"name\"] = ent.text\n",
    "            break\n",
    "    \n",
    "    # Extract social links\n",
    "    social_pattern = r'(https?://\\S+(?:linkedin\\.com|github\\.com|twitter\\.com)\\S+)'\n",
    "    personal_info[\"social\"] = re.findall(social_pattern, doc.text)\n",
    "    \n",
    "    return personal_info\n",
    "\n",
    "def extract_skills(doc):\n",
    "    \"\"\"\n",
    "    Extract skills using spaCy and regex\n",
    "    \"\"\"\n",
    "    # Common skill keywords\n",
    "    skill_keywords = [\n",
    "        \"python\", \"javascript\", \"java\", \"c++\", \"sql\", \"react\", \n",
    "        \"machine learning\", \"data analysis\", \"docker\", \"git\",\n",
    "        \"tensorflow\", \"pandas\", \"numpy\", \"scikit-learn\",\n",
    "        \"html\", \"css\", \"node.js\", \"cloud computing\"\n",
    "    ]\n",
    "    \n",
    "    skills = []\n",
    "    for token in doc:\n",
    "        # Check for skill keywords\n",
    "        if token.text.lower() in skill_keywords:\n",
    "            skills.append({\n",
    "                \"name\": token.text.capitalize(),\n",
    "                \"type\": \"technical\",\n",
    "                \"experience\": 0  # To be refined later\n",
    "            })\n",
    "    \n",
    "    return skills\n",
    "\n",
    "def extract_education(doc):\n",
    "    \"\"\"\n",
    "    Extract education information\n",
    "    \"\"\"\n",
    "    education = []\n",
    "    \n",
    "    # Look for education-related entities and patterns\n",
    "    edu_patterns = [\n",
    "        r'(Bachelor|Master|PhD|Doctorate)[\\s]+(of|in)?[\\s]*([\\w\\s]+)',\n",
    "        r'(B\\.?A\\.?|B\\.?S\\.?|M\\.?S\\.?|M\\.?A\\.?|Ph\\.?D\\.?)\\s+(in)?[\\s]*([\\w\\s]+)'\n",
    "    ]\n",
    "    \n",
    "    for pattern in edu_patterns:\n",
    "        matches = re.findall(pattern, doc.text, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            degree = match[0]\n",
    "            field = match[2] if len(match) > 2 else \"\"\n",
    "            education.append({\n",
    "                \"degree\": degree,\n",
    "                \"institution\": \"\",  # To be refined\n",
    "                \"dates\": \"\"  # To be refined\n",
    "            })\n",
    "    \n",
    "    return education\n",
    "\n",
    "def extract_experience(doc):\n",
    "    \"\"\"\n",
    "    Extract work experience details\n",
    "    \"\"\"\n",
    "    experience = []\n",
    "    \n",
    "    # Patterns for job titles and companies\n",
    "    job_patterns = [\n",
    "        r'([\\w\\s]+)\\s+at\\s+([\\w\\s]+)',\n",
    "        r'([\\w\\s]+)\\s+in\\s+([\\w\\s]+)\\s+company'\n",
    "    ]\n",
    "    \n",
    "    for pattern in job_patterns:\n",
    "        matches = re.findall(pattern, doc.text, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            experience.append({\n",
    "                \"title\": match[0],\n",
    "                \"company\": match[1],\n",
    "                \"dates\": \"\",  # To be refined\n",
    "                \"location\": \"\"  # To be refined\n",
    "            })\n",
    "    \n",
    "    return experience\n",
    "\n",
    "def validate_with_ai(extracted_data, resume_text):\n",
    "    \"\"\"\n",
    "    Validate and refine extracted data using AI\n",
    "    \"\"\"\n",
    "    # Set the model to 'models/gemini-1.5-flash-001'\n",
    "    model_to_use = 'models/gemini-1.5-flash-001'\n",
    "    \n",
    "    try:\n",
    "        # Initialize the model\n",
    "        model = genai.GenerativeModel(model_to_use)\n",
    "        \n",
    "        # Prepare prompt\n",
    "        prompt = f\"\"\"Carefully review and validate the extracted resume information. \n",
    "        Provide a refined, complete JSON representation of the resume details.\n",
    "\n",
    "        Extracted Data:\n",
    "        {json.dumps(extracted_data)}\n",
    "\n",
    "        Original Resume Text:\n",
    "        {resume_text}\n",
    "\n",
    "        Requirements:\n",
    "        - Validate and correct extracted information\n",
    "        - Fill in missing details\n",
    "        - Ensure JSON is complete and accurate\n",
    "        - Add reasonable estimates for missing fields\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate content\n",
    "        response = model.generate_content(prompt)\n",
    "        \n",
    "        # Clean and parse the response\n",
    "        json_text = response.text.strip('```json').strip('```').strip()\n",
    "        validated_data = json.loads(json_text)\n",
    "        return validated_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"AI validation error: {e}\")\n",
    "        return extracted_data\n",
    "\n",
    "\n",
    "def process_resume(resume_file, job_role):\n",
    "    \"\"\"\n",
    "    Main resume processing function\n",
    "    \"\"\"\n",
    "    # Extract text from file\n",
    "    resume_text = extract_text_from_file(resume_file.name)\n",
    "    \n",
    "    # Process with spaCy\n",
    "    doc = nlp(resume_text)\n",
    "    \n",
    "    # Extract initial information\n",
    "    extracted_data = {\n",
    "        \"personal\": extract_personal_info(doc),\n",
    "        \"skills\": extract_skills(doc),\n",
    "        \"education\": extract_education(doc),\n",
    "        \"experience\": extract_experience(doc),\n",
    "        \"overall_experience\": 0,\n",
    "        \"salary\": 0.0\n",
    "    }\n",
    "    \n",
    "    # Validate and refine with AI\n",
    "    refined_data = validate_with_ai(extracted_data, resume_text)\n",
    "    \n",
    "    # Match job description\n",
    "    job_match_result = match_job_description(refined_data, job_role)\n",
    "    \n",
    "    # Combine results\n",
    "    final_result = {\n",
    "        \"resume_details\": refined_data,\n",
    "        \"job_match\": job_match_result\n",
    "    }\n",
    "    \n",
    "    return json.dumps(final_result, indent=2)\n",
    "\n",
    "def match_job_description(resume_data, job_role):\n",
    "    \"\"\"\n",
    "    Match resume skills with job description\n",
    "    \"\"\"\n",
    "    job_description = JOB_DESCRIPTIONS.get(job_role, {})\n",
    "    required_skills = job_description.get('required_skills', [])\n",
    "    \n",
    "    # Extract skill names\n",
    "    resume_skills = [\n",
    "        skill.get('name', '').lower() \n",
    "        for skill in resume_data.get('skills', [])\n",
    "    ]\n",
    "    \n",
    "    # Calculate matching skills\n",
    "    matched_skills = [\n",
    "        skill for skill in required_skills \n",
    "        if skill.lower() in resume_skills\n",
    "    ]\n",
    "    unmatched_skills = list(set(required_skills) - set(matched_skills))\n",
    "    \n",
    "    # Calculate match score\n",
    "    match_percentage = (len(matched_skills) / len(required_skills)) * 100 if required_skills else 0\n",
    "    \n",
    "    return {\n",
    "        \"match_score\": round(match_percentage, 2),\n",
    "        \"matched_skills\": matched_skills,\n",
    "        \"unmatched_skills\": unmatched_skills,\n",
    "        \"job_description\": job_description.get('description', '')\n",
    "    }\n",
    "\n",
    "# Gradio Interface\n",
    "def create_gradio_interface():\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"# Advanced Resume Extraction\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            resume_input = gr.File(label=\"Upload Resume\")\n",
    "            job_role_dropdown = gr.Dropdown(\n",
    "                choices=list(JOB_DESCRIPTIONS.keys()), \n",
    "                label=\"Select Job Role\"\n",
    "            )\n",
    "        \n",
    "        submit_btn = gr.Button(\"Process Resume\")\n",
    "        output = gr.TextArea(label=\"Results\")\n",
    "        \n",
    "        submit_btn.click(\n",
    "            fn=process_resume, \n",
    "            inputs=[resume_input, job_role_dropdown], \n",
    "            outputs=output\n",
    "        )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "# Launch the app\n",
    "if __name__ == \"__main__\":\n",
    "    # First, verify available models\n",
    "    available_models = get_available_models()\n",
    "    \n",
    "    # Launch Gradio interface\n",
    "    demo = create_gradio_interface()\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5317195f-3c27-4aeb-9162-5f82d4709bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Models: ['models/gemini-1.0-pro-vision-latest', 'models/gemini-pro-vision', 'models/gemini-1.5-pro-latest', 'models/gemini-1.5-pro-001', 'models/gemini-1.5-pro-002', 'models/gemini-1.5-pro', 'models/gemini-1.5-flash-latest', 'models/gemini-1.5-flash-001', 'models/gemini-1.5-flash-001-tuning', 'models/gemini-1.5-flash', 'models/gemini-1.5-flash-002', 'models/gemini-1.5-flash-8b', 'models/gemini-1.5-flash-8b-001', 'models/gemini-1.5-flash-8b-latest', 'models/gemini-1.5-flash-8b-exp-0827', 'models/gemini-1.5-flash-8b-exp-0924', 'models/gemini-2.0-flash-exp', 'models/gemini-2.0-flash', 'models/gemini-2.0-flash-001', 'models/gemini-2.0-flash-exp-image-generation', 'models/gemini-2.0-flash-lite-001', 'models/gemini-2.0-flash-lite', 'models/gemini-2.0-flash-lite-preview-02-05', 'models/gemini-2.0-flash-lite-preview', 'models/gemini-2.0-pro-exp', 'models/gemini-2.0-pro-exp-02-05', 'models/gemini-exp-1206', 'models/gemini-2.0-flash-thinking-exp-01-21', 'models/gemini-2.0-flash-thinking-exp', 'models/gemini-2.0-flash-thinking-exp-1219', 'models/learnlm-1.5-pro-experimental', 'models/gemma-3-27b-it']\n",
      "* Running on local URL:  http://127.0.0.1:7878\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7878/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response: ```json\n",
      "{\n",
      "  \"personal\": {\n",
      "    \"name\": \"Venkata Surya Vishal Ganti\",\n",
      "    \"email\": \"suryavishal2002@gmail.com\",\n",
      "    \"phone\": \"+91 8500031155\",\n",
      "    \"address\": \"Srikakulam, Andhra Pradesh 532005\",\n",
      "    \"social\": [\n",
      "      \"https://www.linkedin.com/in/surya-vishal-ganti/\"\n",
      "    ]\n",
      "  },\n",
      "  \"skills\": [\n",
      "    {\n",
      "      \"name\": \"Python\",\n",
      "      \"type\": \"technical\",\n",
      "      \"experience\": 2\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"SQL\",\n",
      "      \"type\": \"technical\",\n",
      "      \"experience\": 1\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Pandas\",\n",
      "      \"type\": \"technical\",\n",
      "      \"experience\": 1\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"NumPy\",\n",
      "      \"type\": \"technical\",\n",
      "      \"experience\": 2\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Scikit-Learn\",\n",
      "      \"type\": \"technical\",\n",
      "      \"experience\": 1\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Matplotlib\",\n",
      "      \"type\": \"technical\",\n",
      "      \"experience\": 1\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"OpenCV\",\n",
      "      \"type\": \"technical\",\n",
      "      \"experience\": 2\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"TensorFlow\",\n",
      "      \"type\": \"technical\",\n",
      "      \"experience\": 2\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Machine Learning\",\n",
      "      \"type\": \"technical\",\n",
      "      \"experience\": 2\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Deep Learning\",\n",
      "      \"type\": \"technical\",\n",
      "      \"experience\": 2\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"PyCharm\",\n",
      "      \"type\": \"technical\",\n",
      "      \"experience\": 1\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Jupyter Notebook\",\n",
      "      \"type\": \"technical\",\n",
      "      \"experience\": 1\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Visual Studio Code\",\n",
      "      \"type\": \"technical\",\n",
      "      \"experience\": 1\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Google Colab\",\n",
      "      \"type\": \"technical\",\n",
      "      \"experience\": 1\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Communication\",\n",
      "      \"type\": \"soft\",\n",
      "      \"experience\": 0\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Presentation\",\n",
      "      \"type\": \"soft\",\n",
      "      \"experience\": 0\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Teamwork\",\n",
      "      \"type\": \"soft\",\n",
      "      \"experience\": 0\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Problem-Solving\",\n",
      "      \"type\": \"soft\",\n",
      "      \"experience\": 0\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Time Management\",\n",
      "      \"type\": \"soft\",\n",
      "      \"experience\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"education\": [\n",
      "    {\n",
      "      \"degree\": \"B. Tech\",\n",
      "      \"institution\": \"Anil Neerukonda Institute of Technology and Sciences (ANITS)\",\n",
      "      \"dates\": \"November 2021 - April 2024\",\n",
      "      \"major\": \"Electrical and Electronics Engineering\",\n",
      "      \"gpa\": 7.00\n",
      "    },\n",
      "    {\n",
      "      \"degree\": \"Diploma\",\n",
      "      \"institution\": \"Government Polytechnic Srikakulam\",\n",
      "      \"dates\": \"July 2018 - October 2021\",\n",
      "      \"major\": \"Electrical and Electronics Engineering\",\n",
      "      \"gpa\": 75\n",
      "    },\n",
      "    {\n",
      "      \"degree\": \"Schooling\",\n",
      "      \"institution\": \"New Central School\",\n",
      "      \"dates\": \"July 2017 - April 2018\",\n",
      "      \"major\": \"\"\n",
      "    }\n",
      "  ],\n",
      "  \"experience\": [\n",
      "    {\n",
      "      \"title\": \"Graduate Appr entice Developer (GAD)\",\n",
      "      \"company\": \"Tekworks Enterprises\",\n",
      "      \"dates\": \"November 2024 - Present\",\n",
      "      \"location\": \"Vijayawada, Andhra Pradesh\"\n",
      "    }\n",
      "  ],\n",
      "  \"overall_experience\": 0.5,\n",
      "  \"salary\": 0.0,\n",
      "  \"projects\": [\n",
      "    {\n",
      "      \"name\": \"Litzchill Application\",\n",
      "      \"dates\": \"November 2024 - Present\",\n",
      "      \"description\": \"Developed and implemented image inpainting techniques.\\nDesigned and trained machine learning models for face emotion detection.\\nConducted comprehensive image testing and training procedures.\\nAccurately annotated image datasets for improved model performance.\\nEfficiently collected and curated image data from various online sources.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Library Data Analysis Project\",\n",
      "      \"dates\": \"July 2024 - August 2024\",\n",
      "      \"description\": \"Analyzed and interpreted complex library datasets using SQL.\\nDesigned and implemented efficient data tables for optimal data storage and retrieval.\\nSuccessfully imported and managed large datasets within the MySQL environment.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Fortune 500 Data Extraction Project\",\n",
      "      \"dates\": \"June 2024 - July 2024\",\n",
      "      \"description\": \"Performed in-depth web scraping of the Fortune 500 website.\\nConducted thorough data analysis, cleaning, and transformation to ensure data accuracy and reliability.\\nDeveloped insightful data visualizations to communicate key findings and support informed decision-making.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Hotel Menu Application\",\n",
      "      \"dates\": \"May 2024 - June 2024\",\n",
      "      \"description\": \"Developed an innovative Python-based application for automated menu display and management.\\nImplemented a user-friendly interface with detailed item information and descriptions.\\nDesigned and integrated a unique dish serving alarm system to enhance the dining experience.\"\n",
      "    }\n",
      "  ],\n",
      "  \"certifications\": [\n",
      "    {\n",
      "      \"name\": \"Data Analysis Certification\",\n",
      "      \"issuer\": \"Innomatics Research Labs\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Data Science Certification\",\n",
      "      \"issuer\": \"Innomatics Research Labs\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Python Programming Certification\",\n",
      "      \"issuer\": \"Programming Hub\"\n",
      "    }\n",
      "  ],\n",
      "  \"awards\": [],\n",
      "  \"languages\": []\n",
      "}\n",
      "```\n",
      "\n",
      "**Explanation of Changes:**\n",
      "\n",
      "1. **Personal Information:**\n",
      "   - **name:** Corrected to \"Venkata Surya Vishal Ganti\" based on the resume text.\n",
      "   - **address:** Added from the resume text.\n",
      "\n",
      "2. **Skills:**\n",
      "   - **experience:**  Estimated experience levels based on the resume details.\n",
      "   - Added missing skills like \"Matplotlib\" and \"Easy OCR\" from the resume.\n",
      "   - Added \"soft skills\" from the resume text.\n",
      "\n",
      "3. **Education:**\n",
      "   - **degree:** Corrected \"ma\", \"ms\", and \"bs\" to actual degrees based on the resume text.\n",
      "   - **institution:** Added institution names.\n",
      "   - **dates:** Added dates from the resume text.\n",
      "   - **major:** Added majors based on the resume text.\n",
      "   - **gpa:** Added GPA information for B. Tech.\n",
      "\n",
      "4. **Experience:**\n",
      "   - **title:** Corrected to \"Graduate Appr entice Developer (GAD)\" from the resume text.\n",
      "   - **company:** Corrected to \"Tekworks Enterprises\" from the resume text.\n",
      "   - **dates:** Added dates from the resume text.\n",
      "   - **location:** Added location from the resume text.\n",
      "\n",
      "5. **Overall Experience:**  Estimated as 0.5 years based on the current work experience.\n",
      "\n",
      "6. **Salary:** This is generally not available on resumes. Set to 0.0.\n",
      "\n",
      "7. **Projects:** \n",
      "   - Added \"Projects\" section from the resume text.\n",
      "   - **name:** Added project names.\n",
      "   - **dates:** Added project dates.\n",
      "   - **description:** Added project descriptions.\n",
      "\n",
      "8. **Certifications:**\n",
      "   - Added certifications from the resume text.\n",
      "\n",
      "9. **Awards:**  This is not mentioned in the resume. Kept as an empty list.\n",
      "\n",
      "10. **Languages:**  This information is not present in the resume. Kept as an empty list.\n",
      "AI validation error: Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import spacy\n",
    "import re\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "import PyPDF2\n",
    "import docx\n",
    "\n",
    "# Configure Gemini API\n",
    "genai.configure(api_key=\"AIzaSyBQYHE915iZLgTibkof6Un5sJmyCFK_IO0\")\n",
    "\n",
    "# Verify and list available models\n",
    "def get_available_models():\n",
    "    \"\"\"\n",
    "    Retrieve and print available generative models\n",
    "    \"\"\"\n",
    "    try:\n",
    "        models = [m.name for m in genai.list_models() \n",
    "                  if 'generateContent' in m.supported_generation_methods]\n",
    "        print(\"Available Models:\", models)\n",
    "        return models\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing models: {e}\")\n",
    "        return []\n",
    "\n",
    "# Load spaCy model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "except OSError:\n",
    "    print(\"Downloading spaCy model...\")\n",
    "    spacy.cli.download(\"en_core_web_lg\")\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Predefined Job Descriptions\n",
    "JOB_DESCRIPTIONS = {\n",
    "    \"Data Scientist\": {\n",
    "        \"required_skills\": [\n",
    "            \"Python\", \"Machine Learning\", \"Data Analysis\", \n",
    "            \"SQL\", \"Pandas\", \"NumPy\", \"TensorFlow\", \"scikit-learn\"\n",
    "        ],\n",
    "        \"description\": \"Analyze complex data sets, develop machine learning models, and provide actionable insights.\"\n",
    "    },\n",
    "    \"Software Engineer\": {\n",
    "        \"required_skills\": [\n",
    "            \"Python\", \"JavaScript\", \"React\", \"Node.js\", \n",
    "            \"SQL\", \"Git\", \"Docker\", \"Cloud Computing\"\n",
    "        ],\n",
    "        \"description\": \"Develop and maintain software applications, collaborate with cross-functional teams, and implement new features.\"\n",
    "    },\n",
    "    \"Frontend Developer\": {\n",
    "        \"required_skills\": [\n",
    "            \"JavaScript\", \"React\", \"HTML\", \"CSS\", \n",
    "            \"TypeScript\", \"Vue.js\", \"Angular\", \"Responsive Design\"\n",
    "        ],\n",
    "        \"description\": \"Create responsive and interactive web applications with a focus on user experience.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def extract_text_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Extract text from various file types\n",
    "    \"\"\"\n",
    "    file_extension = file_path.split('.')[-1].lower()\n",
    "    \n",
    "    try:\n",
    "        # PDF handling\n",
    "        if file_extension == 'pdf':\n",
    "            with open(file_path, 'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                text = ''\n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text() or ''\n",
    "                return text\n",
    "        \n",
    "        # DOCX handling\n",
    "        elif file_extension == 'docx':\n",
    "            doc = docx.Document(file_path)\n",
    "            return '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
    "        \n",
    "        # Text file handling\n",
    "        else:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                return file.read()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_personal_info(doc):\n",
    "    \"\"\"\n",
    "    Extract personal information using spaCy\n",
    "    \"\"\"\n",
    "    personal_info = {\n",
    "        \"name\": \"\",\n",
    "        \"email\": \"\",\n",
    "        \"phone\": \"\",\n",
    "        \"address\": \"\",\n",
    "        \"social\": []\n",
    "    }\n",
    "    \n",
    "    # Extract email\n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "    emails = re.findall(email_pattern, doc.text)\n",
    "    personal_info[\"email\"] = emails[0] if emails else \"\"\n",
    "    \n",
    "    # Extract phone number\n",
    "    phone_pattern = r'\\b(?:\\+\\d{1,2}\\s?)?(?:\\(\\d{3}\\)|\\d{3})[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b'\n",
    "    phones = re.findall(phone_pattern, doc.text)\n",
    "    personal_info[\"phone\"] = phones[0] if phones else \"\"\n",
    "    \n",
    "    # Extract name (using the first person entity)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\" and len(ent.text.split()) > 1:\n",
    "            personal_info[\"name\"] = ent.text\n",
    "            break\n",
    "    \n",
    "    # Extract social links\n",
    "    social_pattern = r'(https?://\\S+(?:linkedin\\.com|github\\.com|twitter\\.com)\\S+)'\n",
    "    personal_info[\"social\"] = re.findall(social_pattern, doc.text)\n",
    "    \n",
    "    return personal_info\n",
    "\n",
    "def extract_skills(doc):\n",
    "    \"\"\"\n",
    "    Extract skills using spaCy and regex\n",
    "    \"\"\"\n",
    "    # Common skill keywords\n",
    "    skill_keywords = [\n",
    "        \"python\", \"javascript\", \"java\", \"c++\", \"sql\", \"react\", \n",
    "        \"machine learning\", \"data analysis\", \"docker\", \"git\",\n",
    "        \"tensorflow\", \"pandas\", \"numpy\", \"scikit-learn\",\n",
    "        \"html\", \"css\", \"node.js\", \"cloud computing\"\n",
    "    ]\n",
    "    \n",
    "    skills = []\n",
    "    for token in doc:\n",
    "        # Check for skill keywords\n",
    "        if token.text.lower() in skill_keywords:\n",
    "            skills.append({\n",
    "                \"name\": token.text.capitalize(),\n",
    "                \"type\": \"technical\",\n",
    "                \"experience\": 0  # To be refined later\n",
    "            })\n",
    "    \n",
    "    return skills\n",
    "\n",
    "def extract_education(doc):\n",
    "    \"\"\"\n",
    "    Extract education information\n",
    "    \"\"\"\n",
    "    education = []\n",
    "    \n",
    "    # Look for education-related entities and patterns\n",
    "    edu_patterns = [\n",
    "        r'(Bachelor|Master|PhD|Doctorate)[\\s]+(of|in)?[\\s]*([\\w\\s]+)',\n",
    "        r'(B\\.?A\\.?|B\\.?S\\.?|M\\.?S\\.?|M\\.?A\\.?|Ph\\.?D\\.?)\\s+(in)?[\\s]*([\\w\\s]+)'\n",
    "    ]\n",
    "    \n",
    "    for pattern in edu_patterns:\n",
    "        matches = re.findall(pattern, doc.text, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            degree = match[0]\n",
    "            field = match[2] if len(match) > 2 else \"\"\n",
    "            education.append({\n",
    "                \"degree\": degree,\n",
    "                \"institution\": \"\",  # To be refined\n",
    "                \"dates\": \"\"  # To be refined\n",
    "            })\n",
    "    \n",
    "    return education\n",
    "\n",
    "def extract_experience(doc):\n",
    "    \"\"\"\n",
    "    Extract work experience details\n",
    "    \"\"\"\n",
    "    experience = []\n",
    "    \n",
    "    # Patterns for job titles and companies\n",
    "    job_patterns = [\n",
    "        r'([\\w\\s]+)\\s+at\\s+([\\w\\s]+)',\n",
    "        r'([\\w\\s]+)\\s+in\\s+([\\w\\s]+)\\s+company'\n",
    "    ]\n",
    "    \n",
    "    for pattern in job_patterns:\n",
    "        matches = re.findall(pattern, doc.text, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            experience.append({\n",
    "                \"title\": match[0],\n",
    "                \"company\": match[1],\n",
    "                \"dates\": \"\",  # To be refined\n",
    "                \"location\": \"\"  # To be refined\n",
    "            })\n",
    "    \n",
    "    return experience\n",
    "\n",
    "def validate_with_ai(extracted_data, resume_text):\n",
    "    \"\"\"\n",
    "    Validate and refine extracted data using AI\n",
    "    \"\"\"\n",
    "    # Set the model to 'models/gemini-1.5-flash-001'\n",
    "    model_to_use = 'models/gemini-1.5-flash-001'\n",
    "    \n",
    "    try:\n",
    "        # Initialize the model\n",
    "        model = genai.GenerativeModel(model_to_use)\n",
    "        \n",
    "        # Prepare prompt\n",
    "        prompt = f\"\"\"Carefully review and validate the extracted resume information. \n",
    "        Provide a refined, complete JSON representation of the resume details.\n",
    "\n",
    "        Extracted Data:\n",
    "        {json.dumps(extracted_data)}\n",
    "\n",
    "        Original Resume Text:\n",
    "        {resume_text}\n",
    "\n",
    "        Requirements:\n",
    "        - Validate and correct extracted information\n",
    "        - Fill in missing details\n",
    "        - Ensure JSON is complete and accurate\n",
    "        - Add reasonable estimates for missing fields\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate content\n",
    "        response = model.generate_content(prompt)\n",
    "        \n",
    "        # Clean up the response to ensure proper JSON parsing\n",
    "        cleaned_response = response.text.strip()\n",
    "        \n",
    "        # Log the cleaned response for debugging\n",
    "        print(f\"Cleaned Response: {cleaned_response}\")\n",
    "        \n",
    "        # Check if the cleaned response is empty or malformed\n",
    "        if not cleaned_response:\n",
    "            print(\"Error: AI response is empty.\")\n",
    "            return extracted_data\n",
    "        \n",
    "        # If the response contains code blocks (e.g., ```json), remove them\n",
    "        if cleaned_response.startswith(\"```json\") and cleaned_response.endswith(\"```\"):\n",
    "            cleaned_response = cleaned_response[7:-3].strip()\n",
    "        \n",
    "        # Parse the cleaned response as JSON\n",
    "        validated_data = json.loads(cleaned_response)\n",
    "        return validated_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"AI validation error: {e}\")\n",
    "        return extracted_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_resume(resume_file, job_role):\n",
    "    \"\"\"\n",
    "    Main resume processing function\n",
    "    \"\"\"\n",
    "    # Extract text from file\n",
    "    resume_text = extract_text_from_file(resume_file.name)\n",
    "    \n",
    "    # Process with spaCy\n",
    "    doc = nlp(resume_text)\n",
    "    \n",
    "    # Extract initial information\n",
    "    extracted_data = {\n",
    "        \"personal\": extract_personal_info(doc),\n",
    "        \"skills\": extract_skills(doc),\n",
    "        \"education\": extract_education(doc),\n",
    "        \"experience\": extract_experience(doc),\n",
    "        \"overall_experience\": 0,\n",
    "        \"salary\": 0.0\n",
    "    }\n",
    "    \n",
    "    # Validate and refine with AI\n",
    "    refined_data = validate_with_ai(extracted_data, resume_text)\n",
    "    \n",
    "    # Match job description\n",
    "    job_match_result = match_job_description(refined_data, job_role)\n",
    "    \n",
    "    # Combine results\n",
    "    final_result = {\n",
    "        \"resume_details\": refined_data,\n",
    "        \"job_match\": job_match_result\n",
    "    }\n",
    "    \n",
    "    return json.dumps(final_result, indent=2)\n",
    "\n",
    "def match_job_description(resume_data, job_role):\n",
    "    \"\"\"\n",
    "    Match resume skills with job description\n",
    "    \"\"\"\n",
    "    job_description = JOB_DESCRIPTIONS.get(job_role, {})\n",
    "    required_skills = job_description.get('required_skills', [])\n",
    "    \n",
    "    # Extract skill names\n",
    "    resume_skills = [\n",
    "        skill.get('name', '').lower() \n",
    "        for skill in resume_data.get('skills', [])\n",
    "    ]\n",
    "    \n",
    "    # Calculate matching skills\n",
    "    matched_skills = [\n",
    "        skill for skill in required_skills \n",
    "        if skill.lower() in resume_skills\n",
    "    ]\n",
    "    unmatched_skills = list(set(required_skills) - set(matched_skills))\n",
    "    \n",
    "    # Calculate match score\n",
    "    match_percentage = (len(matched_skills) / len(required_skills)) * 100 if required_skills else 0\n",
    "    \n",
    "    return {\n",
    "        \"match_score\": round(match_percentage, 2),\n",
    "        \"matched_skills\": matched_skills,\n",
    "        \"unmatched_skills\": unmatched_skills,\n",
    "        \"job_description\": job_description.get('description', '')\n",
    "    }\n",
    "\n",
    "# Gradio Interface\n",
    "def create_gradio_interface():\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"# Advanced Resume Extraction\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            resume_input = gr.File(label=\"Upload Resume\")\n",
    "            job_role_dropdown = gr.Dropdown(\n",
    "                choices=list(JOB_DESCRIPTIONS.keys()), \n",
    "                label=\"Select Job Role\"\n",
    "            )\n",
    "        \n",
    "        submit_btn = gr.Button(\"Process Resume\")\n",
    "        output = gr.TextArea(label=\"Results\")\n",
    "        \n",
    "        submit_btn.click(\n",
    "            fn=process_resume, \n",
    "            inputs=[resume_input, job_role_dropdown], \n",
    "            outputs=output\n",
    "        )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "# Launch the app\n",
    "if __name__ == \"__main__\":\n",
    "    # First, verify available models\n",
    "    available_models = get_available_models()\n",
    "    \n",
    "    # Launch Gradio interface\n",
    "    demo = create_gradio_interface()\n",
    "    demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aaef6783-09cb-4ed4-8f80-8a73511b96f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7879\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7879/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response: ```json\n",
      "{\n",
      "  \"personal\": {\n",
      "    \"name\": \"Venkata Surya Vishal Ganti\",\n",
      "    \"email\": \"suryavishal2002@gmail.com\",\n",
      "    \"phone\": \"+91 8500031155\",\n",
      "    \"address\": \"Srikakulam, Andhra Pradesh 532005\",\n",
      "    \"social\": [\"https://www.linkedin.com/in/surya-vishal-ganti/\"]\n",
      "  },\n",
      "  \"skills\": [\n",
      "    {\n",
      "      \"name\": \"Python\",\n",
      "      \"type\": \"technical\",\n",
      "      \"experience\": 2\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Tensorflow\",\n",
      "      \"type\": \"technical\",\n",
      "      \"experience\": 2\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Numpy\",\n",
      "      \"type\": \"technical\",\n",
      "      \"experience\": 2\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"SQL\",\n",
      "      \"type\": \"technical\",\n",
      "      \"experience\": 1\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Pandas\",\n",
      "      \"type\": \"technical\",\n",
      "      \"experience\": 1\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Scikit-Learn\",\n",
      "      \"type\": \"technical\",\n",
      "      \"experience\": 2\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Matplotlib\",\n",
      "      \"type\": \"technical\",\n",
      "      \"experience\": 1\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"OpenCV\",\n",
      "      \"type\": \"technical\",\n",
      "      \"experience\": 2\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Machine Learning\",\n",
      "      \"type\": \"technical\",\n",
      "      \"experience\": 2\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Deep Learning\",\n",
      "      \"type\": \"technical\",\n",
      "      \"experience\": 2\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"PyCharm\",\n",
      "      \"type\": \"technical\",\n",
      "      \"experience\": 1\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Jupyter Notebook\",\n",
      "      \"type\": \"technical\",\n",
      "      \"experience\": 1\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Visual Studio Code\",\n",
      "      \"type\": \"technical\",\n",
      "      \"experience\": 1\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Google Colab\",\n",
      "      \"type\": \"technical\",\n",
      "      \"experience\": 1\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Communication\",\n",
      "      \"type\": \"soft\",\n",
      "      \"experience\": 0\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Presentation\",\n",
      "      \"type\": \"soft\",\n",
      "      \"experience\": 0\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Teamwork\",\n",
      "      \"type\": \"soft\",\n",
      "      \"experience\": 0\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Problem-Solving\",\n",
      "      \"type\": \"soft\",\n",
      "      \"experience\": 0\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Time Management\",\n",
      "      \"type\": \"soft\",\n",
      "      \"experience\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"education\": [\n",
      "    {\n",
      "      \"degree\": \"B. Tech\",\n",
      "      \"institution\": \"Anil Neerukonda Institute of Technology and Sciences (ANITS)\",\n",
      "      \"dates\": \"November 2021 - April 2024\",\n",
      "      \"major\": \"Electrical and Electronics Engineering\",\n",
      "      \"gpa\": \"7.00\"\n",
      "    },\n",
      "    {\n",
      "      \"degree\": \"Diploma\",\n",
      "      \"institution\": \"Government Polytechnic Srikakulam\",\n",
      "      \"dates\": \"July 2018 - October 2021\",\n",
      "      \"major\": \"Electrical and Electronics Engineering\",\n",
      "      \"gpa\": \"75%\"\n",
      "    },\n",
      "    {\n",
      "      \"degree\": \"Schooling\",\n",
      "      \"institution\": \"New Central School\",\n",
      "      \"dates\": \"July 2017 - April 2018\",\n",
      "      \"major\": null,\n",
      "      \"gpa\": null\n",
      "    }\n",
      "  ],\n",
      "  \"experience\": [\n",
      "    {\n",
      "      \"title\": \"Graduate Apprentice Developer (GAD)\",\n",
      "      \"company\": \"Tekworks Enterprises\",\n",
      "      \"dates\": \"November 2024 - Present\",\n",
      "      \"location\": \"Vijayawada, Andhra Pradesh\"\n",
      "    }\n",
      "  ],\n",
      "  \"projects\": [\n",
      "    {\n",
      "      \"name\": \"Litzchill Application\",\n",
      "      \"dates\": \"November 2024 - Present\",\n",
      "      \"description\": \"Developed and implemented image inpainting techniques. Designed and trained machine learning models for face emotion detection. Conducted comprehensive image testing and training procedures. Accurately annotated image datasets for improved model performance. Efficiently collected and curated image data from various online sources.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Library Data Analysis Project\",\n",
      "      \"dates\": \"July 2024 - August 2024\",\n",
      "      \"description\": \"Analyzed and interpreted complex library datasets using SQL. Designed and implemented efficient data tables for optimal data storage and retrieval. Successfully imported and managed large datasets within the MySQL environment.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Fortune 500 Data Extraction Project\",\n",
      "      \"dates\": \"June 2024 - July 2024\",\n",
      "      \"description\": \"Performed in-depth web scraping of the Fortune 500 website. Conducted thorough data analysis, cleaning, and transformation to ensure data accuracy and reliability. Developed insightful data visualizations to communicate key findings and support informed decision-making.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Hotel Menu Application\",\n",
      "      \"dates\": \"May 2024 - June 2024\",\n",
      "      \"description\": \"Developed an innovative Python-based application for automated menu display and management. Implemented a user-friendly interface with detailed item information and descriptions. Designed and integrated a unique dish serving alarm system to enhance the dining experience.\"\n",
      "    }\n",
      "  ],\n",
      "  \"awards\": [\n",
      "    {\n",
      "      \"name\": \"Data Analysis Certification\",\n",
      "      \"awarder\": \"Innomatics Research Labs\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Data Science Certification\",\n",
      "      \"awarder\": \"Innomatics Research Labs\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Python Programming Certification\",\n",
      "      \"awarder\": \"Programming Hub\"\n",
      "    }\n",
      "  ],\n",
      "  \"hobbies\": [\n",
      "    \"Coding\",\n",
      "    \"Creative Writing\",\n",
      "    \"Reading\",\n",
      "    \"Technology Exploration\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "**Changes Made:**\n",
      "\n",
      "* **Corrected Name:** Changed \"Vishal Ganti\" to \"Venkata Surya Vishal Ganti\" based on the original text.\n",
      "* **Added Address:** Included the address from the original text.\n",
      "* **Combined Duplicate Skills:** Removed duplicate entries for \"Python\", \"Tensorflow\", and \"Numpy\".\n",
      "* **Added Major and GPA/Percentage:** Added the major and GPA/Percentage for each educational entry.\n",
      "* **Added Company Name:**  Included the company name \"Tekworks Enterprises\" in the work experience. \n",
      "* **Clarified Experience Title:** Changed the vague title to \"Graduate Apprentice Developer (GAD)\" to be more specific.\n",
      "* **Combined Projects:** Combined the similar projects into one entry. \n",
      "* **Added Description for Projects:**  Provided brief descriptions for each project.\n",
      "* **Added Awards:**  Included the certificates mentioned as awards.\n",
      "* **Added Hobbies:** Extracted hobbies from the text.\n",
      "* **Estimated Experience Levels:**  Added experience levels for each skill based on the resume text (estimating 2 years for skills used in multiple projects, 1 year for skills used in one project, and 0 years for skills mentioned but not used in a project). \n",
      "* **Added Location to Experience:** Added the location from the resume text to the work experience. \n",
      "\n",
      "This JSON representation is now more complete and accurate, providing a better picture of the applicant's qualifications.\n",
      "AI validation error: Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import spacy\n",
    "import re\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "import PyPDF2\n",
    "import docx\n",
    "\n",
    "# Configure Gemini API\n",
    "genai.configure(api_key=\"AIzaSyDS0SXbtLKaawt2IdjTezO8HzsaSoM6RJM\")\n",
    "\n",
    "# Verify and list available models\n",
    "def get_available_models():\n",
    "    try:\n",
    "        models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]\n",
    "        print(\"Available Models:\", models)\n",
    "        return models\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing models: {e}\")\n",
    "        return []\n",
    "\n",
    "# Load spaCy model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "except OSError:\n",
    "    print(\"Downloading spaCy model...\")\n",
    "    spacy.cli.download(\"en_core_web_lg\")\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Predefined Job Descriptions\n",
    "JOB_DESCRIPTIONS = {\n",
    "    \"Data Scientist\": {\n",
    "        \"required_skills\": [\n",
    "            \"Python\", \"Machine Learning\", \"Data Analysis\", \n",
    "            \"SQL\", \"Pandas\", \"NumPy\", \"TensorFlow\", \"scikit-learn\"\n",
    "        ],\n",
    "        \"description\": \"Analyze complex data sets, develop machine learning models, and provide actionable insights.\"\n",
    "    },\n",
    "    \"Software Engineer\": {\n",
    "        \"required_skills\": [\n",
    "            \"Python\", \"JavaScript\", \"React\", \"Node.js\", \n",
    "            \"SQL\", \"Git\", \"Docker\", \"Cloud Computing\"\n",
    "        ],\n",
    "        \"description\": \"Develop and maintain software applications, collaborate with cross-functional teams, and implement new features.\"\n",
    "    },\n",
    "    \"Frontend Developer\": {\n",
    "        \"required_skills\": [\n",
    "            \"JavaScript\", \"React\", \"HTML\", \"CSS\", \n",
    "            \"TypeScript\", \"Vue.js\", \"Angular\", \"Responsive Design\"\n",
    "        ],\n",
    "        \"description\": \"Create responsive and interactive web applications with a focus on user experience.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def extract_text_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Extract text from various file types\n",
    "    \"\"\"\n",
    "    file_extension = file_path.split('.')[-1].lower()\n",
    "    \n",
    "    try:\n",
    "        # PDF handling\n",
    "        if file_extension == 'pdf':\n",
    "            with open(file_path, 'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                text = ''\n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text() or ''\n",
    "                return text\n",
    "        \n",
    "        # DOCX handling\n",
    "        elif file_extension == 'docx':\n",
    "            doc = docx.Document(file_path)\n",
    "            return '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
    "        \n",
    "        # Text file handling\n",
    "        else:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                return file.read()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_personal_info(doc):\n",
    "    \"\"\"\n",
    "    Extract personal information using spaCy\n",
    "    \"\"\"\n",
    "    personal_info = {\n",
    "        \"name\": \"\",\n",
    "        \"email\": \"\",\n",
    "        \"phone\": \"\",\n",
    "        \"address\": \"\",\n",
    "        \"social\": []\n",
    "    }\n",
    "    \n",
    "    # Extract email\n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "    emails = re.findall(email_pattern, doc.text)\n",
    "    personal_info[\"email\"] = emails[0] if emails else \"\"\n",
    "    \n",
    "    # Extract phone number\n",
    "    phone_pattern = r'\\b(?:\\+\\d{1,2}\\s?)?(?:\\(\\d{3}\\)|\\d{3})[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b'\n",
    "    phones = re.findall(phone_pattern, doc.text)\n",
    "    personal_info[\"phone\"] = phones[0] if phones else \"\"\n",
    "    \n",
    "    # Extract name (using the first person entity)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\" and len(ent.text.split()) > 1:\n",
    "            personal_info[\"name\"] = ent.text\n",
    "            break\n",
    "    \n",
    "    # Extract social links\n",
    "    social_pattern = r'(https?://\\S+(?:linkedin\\.com|github\\.com|twitter\\.com)\\S+)'\n",
    "    personal_info[\"social\"] = re.findall(social_pattern, doc.text)\n",
    "    \n",
    "    return personal_info\n",
    "\n",
    "def extract_skills(doc):\n",
    "    \"\"\"\n",
    "    Extract skills using spaCy and regex\n",
    "    \"\"\"\n",
    "    # Common skill keywords\n",
    "    skill_keywords = [\n",
    "        \"python\", \"javascript\", \"java\", \"c++\", \"sql\", \"react\", \n",
    "        \"machine learning\", \"data analysis\", \"docker\", \"git\",\n",
    "        \"tensorflow\", \"pandas\", \"numpy\", \"scikit-learn\",\n",
    "        \"html\", \"css\", \"node.js\", \"cloud computing\"\n",
    "    ]\n",
    "    \n",
    "    skills = []\n",
    "    for token in doc:\n",
    "        # Check for skill keywords\n",
    "        if token.text.lower() in skill_keywords:\n",
    "            skills.append({\n",
    "                \"name\": token.text.capitalize(),\n",
    "                \"type\": \"technical\",\n",
    "                \"experience\": 0  # To be refined later\n",
    "            })\n",
    "    \n",
    "    return skills\n",
    "\n",
    "def extract_education(doc):\n",
    "    \"\"\"\n",
    "    Extract education information\n",
    "    \"\"\"\n",
    "    education = []\n",
    "    \n",
    "    # Look for education-related entities and patterns\n",
    "    edu_patterns = [\n",
    "        r'(Bachelor|Master|PhD|Doctorate)[\\s]+(of|in)?[\\s]*([\\w\\s]+)',\n",
    "        r'(B\\.?A\\.?|B\\.?S\\.?|M\\.?S\\.?|M\\.?A\\.?|Ph\\.?D\\.?)\\s+(in)?[\\s]*([\\w\\s]+)'\n",
    "    ]\n",
    "    \n",
    "    for pattern in edu_patterns:\n",
    "        matches = re.findall(pattern, doc.text, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            degree = match[0]\n",
    "            field = match[2] if len(match) > 2 else \"\"\n",
    "            education.append({\n",
    "                \"degree\": degree,\n",
    "                \"institution\": \"\",  # To be refined\n",
    "                \"dates\": \"\"  # To be refined\n",
    "            })\n",
    "    \n",
    "    return education\n",
    "\n",
    "def extract_experience(doc):\n",
    "    \"\"\"\n",
    "    Extract work experience details\n",
    "    \"\"\"\n",
    "    experience = []\n",
    "    \n",
    "    # Patterns for job titles and companies\n",
    "    job_patterns = [\n",
    "        r'([\\w\\s]+)\\s+at\\s+([\\w\\s]+)',\n",
    "        r'([\\w\\s]+)\\s+in\\s+([\\w\\s]+)\\s+company'\n",
    "    ]\n",
    "    \n",
    "    for pattern in job_patterns:\n",
    "        matches = re.findall(pattern, doc.text, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            experience.append({\n",
    "                \"title\": match[0],\n",
    "                \"company\": match[1],\n",
    "                \"dates\": \"\",  # To be refined\n",
    "                \"location\": \"\"  # To be refined\n",
    "            })\n",
    "    \n",
    "    return experience\n",
    "\n",
    "def validate_with_ai(extracted_data, resume_text):\n",
    "    \"\"\"\n",
    "    Validate and refine extracted data using AI\n",
    "    \"\"\"\n",
    "    # Set the model to 'models/gemini-1.5-flash-001'\n",
    "    model_to_use = 'models/gemini-1.5-flash-001'\n",
    "    \n",
    "    try:\n",
    "        # Initialize the model\n",
    "        model = genai.GenerativeModel(model_to_use)\n",
    "        \n",
    "        # Prepare prompt\n",
    "        prompt = f\"\"\"Carefully review and validate the extracted resume information. \n",
    "        Provide a refined, complete JSON representation of the resume details.\n",
    "\n",
    "        Extracted Data:\n",
    "        {json.dumps(extracted_data)}\n",
    "\n",
    "        Original Resume Text:\n",
    "        {resume_text}\n",
    "\n",
    "        Requirements:\n",
    "        - Validate and correct extracted information\n",
    "        - Fill in missing details\n",
    "        - Ensure JSON is complete and accurate\n",
    "        - Add reasonable estimates for missing fields\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate content\n",
    "        response = model.generate_content(prompt)\n",
    "        \n",
    "        # Clean up the response to ensure proper JSON parsing\n",
    "        cleaned_response = response.text.strip()\n",
    "        \n",
    "        # Log the cleaned response for debugging\n",
    "        print(f\"Cleaned Response: {cleaned_response}\")\n",
    "        \n",
    "        # Check if the cleaned response is empty or malformed\n",
    "        if not cleaned_response:\n",
    "            print(\"Error: AI response is empty.\")\n",
    "            return extracted_data\n",
    "        \n",
    "        # If the response contains code blocks (e.g., ```json), remove them\n",
    "        if cleaned_response.startswith(\"```json\") and cleaned_response.endswith(\"```\"):\n",
    "            cleaned_response = cleaned_response[7:-3].strip()\n",
    "        \n",
    "        # Parse the cleaned response as JSON\n",
    "        validated_data = json.loads(cleaned_response)\n",
    "        return validated_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"AI validation error: {e}\")\n",
    "        return extracted_data\n",
    "\n",
    "def process_resume(resume_file, job_role):\n",
    "    \"\"\"\n",
    "    Main resume processing function\n",
    "    \"\"\"\n",
    "    # Extract text from file\n",
    "    resume_text = extract_text_from_file(resume_file.name)\n",
    "    \n",
    "    # Process with spaCy\n",
    "    doc = nlp(resume_text)\n",
    "    \n",
    "    # Extract initial information\n",
    "    extracted_data = {\n",
    "        \"personal\": extract_personal_info(doc),\n",
    "        \"skills\": extract_skills(doc),\n",
    "        \"education\": extract_education(doc),\n",
    "        \"experience\": extract_experience(doc)\n",
    "    }\n",
    "    \n",
    "    # Validate with AI\n",
    "    validated_data = validate_with_ai(extracted_data, resume_text)\n",
    "    \n",
    "    # Match against job descriptions\n",
    "    job_description = JOB_DESCRIPTIONS.get(job_role)\n",
    "    if job_description:\n",
    "        validated_data[\"job_description\"] = job_description[\"description\"]\n",
    "        validated_data[\"matched_skills\"] = [\n",
    "            skill for skill in validated_data[\"skills\"] if skill[\"name\"].lower() in [s.lower() for s in job_description[\"required_skills\"]]\n",
    "        ]\n",
    "    \n",
    "    return validated_data\n",
    "\n",
    "# Gradio interface\n",
    "def resume_interface(resume_file, job_role):\n",
    "    result = process_resume(resume_file, job_role)\n",
    "    return result\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=resume_interface,\n",
    "    inputs=[gr.File(label=\"Upload Resume\"), gr.Dropdown(list(JOB_DESCRIPTIONS.keys()), label=\"Select Job Role\")],\n",
    "    outputs=\"json\",\n",
    "    live=True\n",
    ")\n",
    "\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7b63e2d-9914-42df-9e03-dcfed3b5f1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7882\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7882/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/blocks.py\", line 2113, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/blocks.py\", line 1869, in postprocess_data\n",
      "    self.validate_outputs(block_fn, predictions)  # type: ignore\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/blocks.py\", line 1824, in validate_outputs\n",
      "    raise ValueError(\n",
      "ValueError: A  function (process_resume) didn't return enough output values (needed: 3, returned: 1).\n",
      "    Output components:\n",
      "        [json, json, number]\n",
      "    Output values returned:\n",
      "        [{'candidate_info': {'personal': {'name': 'Venkata Surya Vishal Ganti', 'email': 'suryavishal2002@gmail.com', 'phone': '+91 8500031155', 'address': 'Srikakulam, Andhra Pradesh 532005', 'social': ['https://www.linkedin.com/in/surya-vishal-ganti/']}, 'skills': [{'name': 'Python', 'type': 'technical', 'experience': 0}, {'name': 'SQL', 'type': 'technical', 'experience': 0}, {'name': 'Pandas', 'type': 'technical', 'experience': 0}, {'name': 'NumPy', 'type': 'technical', 'experience': 0}, {'name': 'Scikit-learn', 'type': 'technical', 'experience': 0}, {'name': 'Matplotlib', 'type': 'technical', 'experience': 0}, {'name': 'OpenCV', 'type': 'technical', 'experience': 0}, {'name': 'TensorFlow', 'type': 'technical', 'experience': 0}, {'name': 'Machine Learning', 'type': 'technical', 'experience': 0}, {'name': 'Deep Learning', 'type': 'technical', 'experience': 0}, {'name': 'PyCharm', 'type': 'technical', 'experience': 0}, {'name': 'Jupyter Notebook', 'type': 'technical', 'experience': 0}, {'name': 'Visual Studio Code', 'type': 'technical', 'experience': 0}, {'name': 'Google Colab', 'type': 'technical', 'experience': 0}, {'name': 'Communication', 'type': 'soft', 'experience': 0}, {'name': 'Presentation', 'type': 'soft', 'experience': 0}, {'name': 'Teamwork', 'type': 'soft', 'experience': 0}, {'name': 'Problem-Solving', 'type': 'soft', 'experience': 0}, {'name': 'Time Management', 'type': 'soft', 'experience': 0}], 'education': [{'degree': 'B.Tech in Electrical and Electronics Engineering', 'institution': 'Anil Neerukonda Institute of Technology and Sciences (ANITS)', 'dates': 'November 2021 - April 2024', 'gpa': '7.00'}, {'degree': 'Diploma in Electrical and Electronics Engineering', 'institution': 'Government Polytechnic Srikakulam', 'dates': 'July 2018 - October 2021', 'percentage': '75%'}, {'degree': 'Schooling', 'institution': 'New Central School', 'dates': 'July 2017 - April 2018'}], 'experience': [{'title': 'Graduate Apprentice Developer (GAD)', 'company': 'Tekworks Enterprises', 'dates': 'November 2024 - Present', 'location': 'Vijayawada, Andhra Pradesh'}], 'job_role': 'Data Scientist'}, 'job_analysis': {'score': 75, 'skill_matches': ['Python', 'Machine Learning', 'TensorFlow', 'NumPy', 'Pandas', 'Scikit-learn', 'SQL'], 'missing_skills': ['Data Analysis'], 'experience_gap': 0.0, 'education_match': False, 'salary_fit': None, 'detailed_breakdown': \"The candidate possesses a strong foundation in Python, Machine Learning, TensorFlow, NumPy, Pandas, Scikit-learn, and SQL, all of which are crucial for the Data Scientist role.  However, explicit experience in 'Data Analysis' is missing from the resume.  The education background is in Electrical and Electronics Engineering, which is not a direct match but the skills acquired show potential.  The experience is limited but relevant, given the candidate is a recent graduate. The overall score reflects the strong skill match but the lack of explicit Data Analysis experience and the mismatch in educational background.\"}, 'match_score': 75}]\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import spacy\n",
    "import re\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "import PyPDF2\n",
    "import docx\n",
    "\n",
    "# Configure Gemini API\n",
    "genai.configure(api_key=\"AIzaSyBQYHE915iZLgTibkof6Un5sJmyCFK_IO0\")\n",
    "\n",
    "# Verify and list available models\n",
    "def get_available_models():\n",
    "    \"\"\"\n",
    "    Retrieve and print available generative models\n",
    "    \"\"\"\n",
    "    try:\n",
    "        models = [m.name for m in genai.list_models() \n",
    "                  if 'generateContent' in m.supported_generation_methods]\n",
    "        print(\"Available Models:\", models)\n",
    "        return models\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing models: {e}\")\n",
    "        return []\n",
    "\n",
    "# Load spaCy model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "except OSError:\n",
    "    print(\"Downloading spaCy model...\")\n",
    "    spacy.cli.download(\"en_core_web_lg\")\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Predefined Job Descriptions\n",
    "JOB_DESCRIPTIONS = {\n",
    "    \"Data Scientist\": {\n",
    "        \"required_skills\": [\n",
    "            \"Python\", \"Machine Learning\", \"Data Analysis\", \n",
    "            \"SQL\", \"Pandas\", \"NumPy\", \"TensorFlow\", \"scikit-learn\"\n",
    "        ],\n",
    "        \"description\": \"Analyze complex data sets, develop machine learning models, and provide actionable insights.\"\n",
    "    },\n",
    "    \"Software Engineer\": {\n",
    "        \"required_skills\": [\n",
    "            \"Python\", \"JavaScript\", \"React\", \"Node.js\", \n",
    "            \"SQL\", \"Git\", \"Docker\", \"Cloud Computing\"\n",
    "        ],\n",
    "        \"description\": \"Develop and maintain software applications, collaborate with cross-functional teams, and implement new features.\"\n",
    "    },\n",
    "    \"Frontend Developer\": {\n",
    "        \"required_skills\": [\n",
    "            \"JavaScript\", \"React\", \"HTML\", \"CSS\", \n",
    "            \"TypeScript\", \"Vue.js\", \"Angular\", \"Responsive Design\"\n",
    "        ],\n",
    "        \"description\": \"Create responsive and interactive web applications with a focus on user experience.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def extract_text_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Extract text from various file types\n",
    "    \"\"\"\n",
    "    file_extension = file_path.split('.')[-1].lower()\n",
    "    \n",
    "    try:\n",
    "        # PDF handling\n",
    "        if file_extension == 'pdf':\n",
    "            with open(file_path, 'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                text = ''\n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text() or ''\n",
    "                return text\n",
    "        \n",
    "        # DOCX handling\n",
    "        elif file_extension == 'docx':\n",
    "            doc = docx.Document(file_path)\n",
    "            return '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
    "        \n",
    "        # Text file handling\n",
    "        else:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                return file.read()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_personal_info(doc):\n",
    "    \"\"\"\n",
    "    Extract personal information using spaCy\n",
    "    \"\"\"\n",
    "    personal_info = {\n",
    "        \"name\": \"\",\n",
    "        \"email\": \"\",\n",
    "        \"phone\": \"\",\n",
    "        \"address\": \"\",\n",
    "        \"social\": []\n",
    "    }\n",
    "    \n",
    "    # Extract email\n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "    emails = re.findall(email_pattern, doc.text)\n",
    "    personal_info[\"email\"] = emails[0] if emails else \"\"\n",
    "    \n",
    "    # Extract phone number\n",
    "    phone_pattern = r'\\b(?:\\+\\d{1,2}\\s?)?(?:\\(\\d{3}\\)|\\d{3})[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b'\n",
    "    phones = re.findall(phone_pattern, doc.text)\n",
    "    personal_info[\"phone\"] = phones[0] if phones else \"\"\n",
    "    \n",
    "    # Extract name (using the first person entity)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\" and len(ent.text.split()) > 1:\n",
    "            personal_info[\"name\"] = ent.text\n",
    "            break\n",
    "    \n",
    "    # Extract social links\n",
    "    social_pattern = r'(https?://\\S+(?:linkedin\\.com|github\\.com|twitter\\.com)\\S+)'\n",
    "    personal_info[\"social\"] = re.findall(social_pattern, doc.text)\n",
    "    \n",
    "    return personal_info\n",
    "\n",
    "def extract_skills(doc):\n",
    "    \"\"\"\n",
    "    Extract skills using spaCy and regex\n",
    "    \"\"\"\n",
    "    # Common skill keywords\n",
    "    skill_keywords = [\n",
    "        \"python\", \"javascript\", \"java\", \"c++\", \"sql\", \"react\", \n",
    "        \"machine learning\", \"data analysis\", \"docker\", \"git\",\n",
    "        \"tensorflow\", \"pandas\", \"numpy\", \"scikit-learn\",\n",
    "        \"html\", \"css\", \"node.js\", \"cloud computing\"\n",
    "    ]\n",
    "    \n",
    "    skills = []\n",
    "    for token in doc:\n",
    "        # Check for skill keywords\n",
    "        if token.text.lower() in skill_keywords:\n",
    "            skills.append({\n",
    "                \"name\": token.text.capitalize(),\n",
    "                \"type\": \"technical\",\n",
    "                \"experience\": 0  # To be refined later\n",
    "            })\n",
    "    \n",
    "    return skills\n",
    "\n",
    "def extract_education(doc):\n",
    "    \"\"\"\n",
    "    Extract education information\n",
    "    \"\"\"\n",
    "    education = []\n",
    "    \n",
    "    # Look for education-related entities and patterns\n",
    "    edu_patterns = [\n",
    "        r'(Bachelor|Master|PhD|Doctorate)[\\s]+(of|in)?[\\s]*([\\w\\s]+)',\n",
    "        r'(B\\.?A\\.?|B\\.?S\\.?|M\\.?S\\.?|M\\.?A\\.?|Ph\\.?D\\.?)\\s+(in)?[\\s]*([\\w\\s]+)'\n",
    "    ]\n",
    "    \n",
    "    for pattern in edu_patterns:\n",
    "        matches = re.findall(pattern, doc.text, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            degree = match[0]\n",
    "            field = match[2] if len(match) > 2 else \"\"\n",
    "            education.append({\n",
    "                \"degree\": degree,\n",
    "                \"institution\": \"\",  # To be refined\n",
    "                \"dates\": \"\"  # To be refined\n",
    "            })\n",
    "    \n",
    "    return education\n",
    "\n",
    "def extract_experience(doc):\n",
    "    \"\"\"\n",
    "    Extract work experience details\n",
    "    \"\"\"\n",
    "    experience = []\n",
    "    \n",
    "    # Patterns for job titles and companies\n",
    "    job_patterns = [\n",
    "        r'([\\w\\s]+)\\s+at\\s+([\\w\\s]+)',\n",
    "        r'([\\w\\s]+)\\s+in\\s+([\\w\\s]+)\\s+company'\n",
    "    ]\n",
    "    \n",
    "    for pattern in job_patterns:\n",
    "        matches = re.findall(pattern, doc.text, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            experience.append({\n",
    "                \"title\": match[0],\n",
    "                \"company\": match[1],\n",
    "                \"dates\": \"\",  # To be refined\n",
    "                \"location\": \"\"  # To be refined\n",
    "            })\n",
    "    \n",
    "    return experience\n",
    "\n",
    "def validate_with_ai(extracted_data, resume_text, job_role):\n",
    "    \"\"\"\n",
    "    Validate and refine extracted data using AI, including JD matching\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "        job_desc = JOB_DESCRIPTIONS.get(job_role, {})\n",
    "        \n",
    "        prompt = f\"\"\"**Resume Analysis Task**\n",
    "        Please perform the following actions:\n",
    "        1. Validate and complete the extracted resume data\n",
    "        2. Compare with this Job Description: {job_desc['description']}\n",
    "        3. Calculate comprehensive match score (0-100)\n",
    "        4. List matched/missing skills\n",
    "        5. Provide detailed matching breakdown\n",
    "\n",
    "        **Input Data:**\n",
    "        {{\n",
    "            \"raw_text\": \"{resume_text[:2000]}...\",\n",
    "            \"extracted_data\": {json.dumps(extracted_data)},\n",
    "            \"job_requirements\": {json.dumps(job_desc)}\n",
    "        }}\n",
    "\n",
    "        **Required Output Format:**\n",
    "        {{\n",
    "            \"validated_data\": {{\n",
    "                \"personal\": {{...}},\n",
    "                \"skills\": [...],\n",
    "                \"education\": [...],\n",
    "                \"experience\": [...]\n",
    "            }},\n",
    "            \"match_analysis\": {{\n",
    "                \"score\": 0-100,\n",
    "                \"skill_matches\": [],\n",
    "                \"missing_skills\": [],\n",
    "                \"experience_gap\": 0.0,\n",
    "                \"education_match\": bool,\n",
    "                \"salary_fit\": bool,\n",
    "                \"detailed_breakdown\": \"text explanation\"\n",
    "            }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "\n",
    "        response = model.generate_content(prompt)\n",
    "        cleaned_response = re.sub(r'```json|```', '', response.text)\n",
    "        return json.loads(cleaned_response.strip())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"AI validation error: {e}\")\n",
    "        return extracted_data\n",
    "\n",
    "def process_resume(resume_file, job_role):\n",
    "    \"\"\"\n",
    "    Enhanced processing with Gemini-powered matching\n",
    "    \"\"\"\n",
    "    resume_text = extract_text_from_file(resume_file.name)\n",
    "    doc = nlp(resume_text)\n",
    "    \n",
    "    # Initial extraction\n",
    "    extracted_data = {\n",
    "        \"personal\": extract_personal_info(doc),\n",
    "        \"skills\": extract_skills(doc),\n",
    "        \"education\": extract_education(doc),\n",
    "        \"experience\": extract_experience(doc),\n",
    "        \"job_role\": job_role\n",
    "    }\n",
    "    \n",
    "    # AI validation and matching\n",
    "    ai_result = validate_with_ai(extracted_data, resume_text, job_role)\n",
    "    \n",
    "    # Format final output\n",
    "    return {\n",
    "        \"candidate_info\": ai_result.get(\"validated_data\", {}),\n",
    "        \"job_analysis\": ai_result.get(\"match_analysis\", {}),\n",
    "        \"match_score\": ai_result.get(\"match_analysis\", {}).get(\"score\", 0)\n",
    "    }\n",
    "\n",
    "# Enhanced Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=process_resume,\n",
    "    inputs=[gr.File(label=\"Upload Resume\"), \n",
    "           gr.Dropdown(choices=list(JOB_DESCRIPTIONS.keys()), label=\"Job Role\")],\n",
    "    outputs=[\n",
    "        gr.JSON(label=\"Validated Resume Data\"),\n",
    "        gr.JSON(label=\"Match Analysis\"),\n",
    "        gr.Number(label=\"Final Match Score\")\n",
    "    ],\n",
    "    title=\"AI-Powered Resume Analyzer\",\n",
    "    description=\"Upload resume and select job role for detailed AI analysis and matching\"\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bba30776-8beb-4ae6-84e7-047cb40ca17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Models: ['models/gemini-1.0-pro-vision-latest', 'models/gemini-pro-vision', 'models/gemini-1.5-pro-latest', 'models/gemini-1.5-pro-001', 'models/gemini-1.5-pro-002', 'models/gemini-1.5-pro', 'models/gemini-1.5-flash-latest', 'models/gemini-1.5-flash-001', 'models/gemini-1.5-flash-001-tuning', 'models/gemini-1.5-flash', 'models/gemini-1.5-flash-002', 'models/gemini-1.5-flash-8b', 'models/gemini-1.5-flash-8b-001', 'models/gemini-1.5-flash-8b-latest', 'models/gemini-1.5-flash-8b-exp-0827', 'models/gemini-1.5-flash-8b-exp-0924', 'models/gemini-2.0-flash-exp', 'models/gemini-2.0-flash', 'models/gemini-2.0-flash-001', 'models/gemini-2.0-flash-exp-image-generation', 'models/gemini-2.0-flash-lite-001', 'models/gemini-2.0-flash-lite', 'models/gemini-2.0-flash-lite-preview-02-05', 'models/gemini-2.0-flash-lite-preview', 'models/gemini-2.0-pro-exp', 'models/gemini-2.0-pro-exp-02-05', 'models/gemini-exp-1206', 'models/gemini-2.0-flash-thinking-exp-01-21', 'models/gemini-2.0-flash-thinking-exp', 'models/gemini-2.0-flash-thinking-exp-1219', 'models/learnlm-1.5-pro-experimental', 'models/gemma-3-27b-it']\n",
      "* Running on local URL:  http://127.0.0.1:7883\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7883/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/blocks.py\", line 2103, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/blocks.py\", line 1650, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nitish/opt/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/utils.py\", line 890, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sd/8gl2mt3j4tnd1_wfnfq6vwz80000gq/T/ipykernel_3251/1710525164.py\", line 250, in process_resume\n",
      "    job_match_result = match_job_description(refined_data, job_role)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sd/8gl2mt3j4tnd1_wfnfq6vwz80000gq/T/ipykernel_3251/1710525164.py\", line 268, in match_job_description\n",
      "    for skill in resume_data.get('skills', [])\n",
      "                 ^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'get'\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import spacy\n",
    "import re\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "import PyPDF2\n",
    "import docx\n",
    "\n",
    "# Configure Gemini API\n",
    "genai.configure(api_key=\"AIzaSyBQYHE915iZLgTibkof6Un5sJmyCFK_IO0\")\n",
    "\n",
    "# Verify and list available models\n",
    "def get_available_models():\n",
    "    \"\"\"\n",
    "    Retrieve and print available generative models\n",
    "    \"\"\"\n",
    "    try:\n",
    "        models = [m.name for m in genai.list_models() \n",
    "                  if 'generateContent' in m.supported_generation_methods]\n",
    "        print(\"Available Models:\", models)\n",
    "        return models\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing models: {e}\")\n",
    "        return []\n",
    "\n",
    "# Load spaCy model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "except OSError:\n",
    "    print(\"Downloading spaCy model...\")\n",
    "    spacy.cli.download(\"en_core_web_lg\")\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Predefined Job Descriptions\n",
    "JOB_DESCRIPTIONS = {\n",
    "    \"Data Scientist\": {\n",
    "        \"required_skills\": [\n",
    "            \"Python\", \"Machine Learning\", \"Data Analysis\", \n",
    "            \"SQL\", \"Pandas\", \"NumPy\", \"TensorFlow\", \"scikit-learn\"\n",
    "        ],\n",
    "        \"description\": \"Analyze complex data sets, develop machine learning models, and provide actionable insights.\"\n",
    "    },\n",
    "    \"Software Engineer\": {\n",
    "        \"required_skills\": [\n",
    "            \"Python\", \"JavaScript\", \"React\", \"Node.js\", \n",
    "            \"SQL\", \"Git\", \"Docker\", \"Cloud Computing\"\n",
    "        ],\n",
    "        \"description\": \"Develop and maintain software applications, collaborate with cross-functional teams, and implement new features.\"\n",
    "    },\n",
    "    \"Frontend Developer\": {\n",
    "        \"required_skills\": [\n",
    "            \"JavaScript\", \"React\", \"HTML\", \"CSS\", \n",
    "            \"TypeScript\", \"Vue.js\", \"Angular\", \"Responsive Design\"\n",
    "        ],\n",
    "        \"description\": \"Create responsive and interactive web applications with a focus on user experience.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def extract_text_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Extract text from various file types\n",
    "    \"\"\"\n",
    "    file_extension = file_path.split('.')[-1].lower()\n",
    "    \n",
    "    try:\n",
    "        # PDF handling\n",
    "        if file_extension == 'pdf':\n",
    "            with open(file_path, 'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                text = ''\n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text() or ''\n",
    "                return text\n",
    "        \n",
    "        # DOCX handling\n",
    "        elif file_extension == 'docx':\n",
    "            doc = docx.Document(file_path)\n",
    "            return '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
    "        \n",
    "        # Text file handling\n",
    "        else:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                return file.read()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_personal_info(doc):\n",
    "    \"\"\"\n",
    "    Extract personal information using spaCy\n",
    "    \"\"\"\n",
    "    personal_info = {\n",
    "        \"name\": \"\",\n",
    "        \"email\": \"\",\n",
    "        \"phone\": \"\",\n",
    "        \"address\": \"\",\n",
    "        \"social\": []\n",
    "    }\n",
    "    \n",
    "    # Extract email\n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "    emails = re.findall(email_pattern, doc.text)\n",
    "    personal_info[\"email\"] = emails[0] if emails else \"\"\n",
    "    \n",
    "    # Extract phone number\n",
    "    phone_pattern = r'\\b(?:\\+\\d{1,2}\\s?)?(?:\\(\\d{3}\\)|\\d{3})[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b'\n",
    "    phones = re.findall(phone_pattern, doc.text)\n",
    "    personal_info[\"phone\"] = phones[0] if phones else \"\"\n",
    "    \n",
    "    # Extract name (using the first person entity)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\" and len(ent.text.split()) > 1:\n",
    "            personal_info[\"name\"] = ent.text\n",
    "            break\n",
    "    \n",
    "    # Extract social links\n",
    "    social_pattern = r'(https?://\\S+(?:linkedin\\.com|github\\.com|twitter\\.com)\\S+)'\n",
    "    personal_info[\"social\"] = re.findall(social_pattern, doc.text)\n",
    "    \n",
    "    return personal_info\n",
    "\n",
    "def extract_skills(doc):\n",
    "    \"\"\"\n",
    "    Extract skills using spaCy and regex\n",
    "    \"\"\"\n",
    "    # Common skill keywords\n",
    "    skill_keywords = [\n",
    "        \"python\", \"javascript\", \"java\", \"c++\", \"sql\", \"react\", \n",
    "        \"machine learning\", \"data analysis\", \"docker\", \"git\",\n",
    "        \"tensorflow\", \"pandas\", \"numpy\", \"scikit-learn\",\n",
    "        \"html\", \"css\", \"node.js\", \"cloud computing\"\n",
    "    ]\n",
    "    \n",
    "    skills = []\n",
    "    for token in doc:\n",
    "        # Check for skill keywords\n",
    "        if token.text.lower() in skill_keywords:\n",
    "            skills.append({\n",
    "                \"name\": token.text.capitalize(),\n",
    "                \"type\": \"technical\",\n",
    "                \"experience\": 0  # To be refined later\n",
    "            })\n",
    "    \n",
    "    return skills\n",
    "\n",
    "def extract_education(doc):\n",
    "    \"\"\"\n",
    "    Extract education information\n",
    "    \"\"\"\n",
    "    education = []\n",
    "    \n",
    "    # Look for education-related entities and patterns\n",
    "    edu_patterns = [\n",
    "        r'(Bachelor|Master|PhD|Doctorate)[\\s]+(of|in)?[\\s]*([\\w\\s]+)',\n",
    "        r'(B\\.?A\\.?|B\\.?S\\.?|M\\.?S\\.?|M\\.?A\\.?|Ph\\.?D\\.?)\\s+(in)?[\\s]*([\\w\\s]+)'\n",
    "    ]\n",
    "    \n",
    "    for pattern in edu_patterns:\n",
    "        matches = re.findall(pattern, doc.text, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            degree = match[0]\n",
    "            field = match[2] if len(match) > 2 else \"\"\n",
    "            education.append({\n",
    "                \"degree\": degree,\n",
    "                \"institution\": \"\",  # To be refined\n",
    "                \"dates\": \"\"  # To be refined\n",
    "            })\n",
    "    \n",
    "    return education\n",
    "\n",
    "def extract_experience(doc):\n",
    "    \"\"\"\n",
    "    Extract work experience details\n",
    "    \"\"\"\n",
    "    experience = []\n",
    "    \n",
    "    # Patterns for job titles and companies\n",
    "    job_patterns = [\n",
    "        r'([\\w\\s]+)\\s+at\\s+([\\w\\s]+)',\n",
    "        r'([\\w\\s]+)\\s+in\\s+([\\w\\s]+)\\s+company'\n",
    "    ]\n",
    "    \n",
    "    for pattern in job_patterns:\n",
    "        matches = re.findall(pattern, doc.text, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            experience.append({\n",
    "                \"title\": match[0],\n",
    "                \"company\": match[1],\n",
    "                \"dates\": \"\",  # To be refined\n",
    "                \"location\": \"\"  # To be refined\n",
    "            })\n",
    "    \n",
    "    return experience\n",
    "\n",
    "def validate_with_ai(extracted_data, resume_text):\n",
    "    # Uses Gemini 1.5 Flash model to validate and refine extracted data\n",
    "    model_to_use = 'models/gemini-1.5-flash-001'\n",
    "    model = genai.GenerativeModel(model_to_use)\n",
    "    \n",
    "    # Prompt asks the AI to:\n",
    "    # - Review extracted information\n",
    "    # - Provide a refined JSON representation\n",
    "    # - Fill in missing details\n",
    "    # - Ensure accuracy\n",
    "    \n",
    "    prompt = f\"\"\"Carefully review and validate the extracted resume information. \n",
    "    Provide a refined, complete JSON representation of the resume details.\n",
    "\n",
    "    Extracted Data:\n",
    "    {json.dumps(extracted_data)}\n",
    "\n",
    "    Original Resume Text:\n",
    "    {resume_text}\n",
    "\n",
    "    Requirements:\n",
    "    - Validate and correct extracted information\n",
    "    - Fill in missing details\n",
    "    - Ensure JSON is complete and accurate\n",
    "    - Add reasonable estimates for missing fields\n",
    "    \"\"\"\n",
    "    \n",
    "    response = model.generate_content(prompt)\n",
    "    # The function then parses and returns the AI-validated dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_resume(resume_file, job_role):\n",
    "    # Extract text from file\n",
    "    resume_text = extract_text_from_file(resume_file.name)\n",
    "    \n",
    "    # Process with spaCy\n",
    "    doc = nlp(resume_text)\n",
    "    \n",
    "    # Extract initial information\n",
    "    extracted_data = {\n",
    "        \"personal\": extract_personal_info(doc),\n",
    "        \"skills\": extract_skills(doc),\n",
    "        \"education\": extract_education(doc),\n",
    "        \"experience\": extract_experience(doc),\n",
    "        \"overall_experience\": 0,\n",
    "        \"salary\": 0.0\n",
    "    }\n",
    "    \n",
    "    # Validate and refine with AI\n",
    "    refined_data = validate_with_ai(extracted_data, resume_text)\n",
    "    \n",
    "    # Match job description\n",
    "    job_match_result = match_job_description(refined_data, job_role)\n",
    "    \n",
    "    # Combine results\n",
    "    final_result = {\n",
    "        \"resume_details\": refined_data,\n",
    "        \"job_match\": job_match_result\n",
    "    }\n",
    "    \n",
    "    return json.dumps(final_result, indent=2)\n",
    "\n",
    "def match_job_description(resume_data, job_role):\n",
    "    # Matches resume skills with job description\n",
    "    job_description = JOB_DESCRIPTIONS.get(job_role, {})\n",
    "    required_skills = job_description.get('required_skills', [])\n",
    "    \n",
    "    # Extract skills from resume\n",
    "    resume_skills = [\n",
    "        skill.get('name', '').lower() \n",
    "        for skill in resume_data.get('skills', [])\n",
    "    ]\n",
    "    \n",
    "    # Calculate matching skills\n",
    "    matched_skills = [\n",
    "        skill for skill in required_skills \n",
    "        if skill.lower() in resume_skills\n",
    "    ]\n",
    "    unmatched_skills = list(set(required_skills) - set(matched_skills))\n",
    "    \n",
    "    # Calculate match score\n",
    "    match_percentage = (len(matched_skills) / len(required_skills)) * 100 if required_skills else 0\n",
    "    \n",
    "    return {\n",
    "        \"match_score\": round(match_percentage, 2),\n",
    "        \"matched_skills\": matched_skills,\n",
    "        \"unmatched_skills\": unmatched_skills,\n",
    "        \"job_description\": job_description.get('description', '')\n",
    "    }\n",
    "\n",
    "# Gradio Interface\n",
    "def create_gradio_interface():\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"# Advanced Resume Extraction\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            resume_input = gr.File(label=\"Upload Resume\")\n",
    "            job_role_dropdown = gr.Dropdown(\n",
    "                choices=list(JOB_DESCRIPTIONS.keys()), \n",
    "                label=\"Select Job Role\"\n",
    "            )\n",
    "        \n",
    "        submit_btn = gr.Button(\"Process Resume\")\n",
    "        output = gr.TextArea(label=\"Results\")\n",
    "        \n",
    "        submit_btn.click(\n",
    "            fn=process_resume, \n",
    "            inputs=[resume_input, job_role_dropdown], \n",
    "            outputs=output\n",
    "        )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "# Launch the app\n",
    "if __name__ == \"__main__\":\n",
    "    # First, verify available models\n",
    "    available_models = get_available_models()\n",
    "    \n",
    "    # Launch Gradio interface\n",
    "    demo = create_gradio_interface()\n",
    "    demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04333dc-6dec-44d9-bce9-b9b37eeb120b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
