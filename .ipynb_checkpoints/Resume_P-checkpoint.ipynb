{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0be361a-3c77-4e26-858b-b894cf9ebc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: nltk in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: pdfplumber in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (0.11.5)\n",
      "Requirement already satisfied: docx2txt in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (0.8)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: torch in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (4.48.3)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (1.9.0.post1)\n",
      "Requirement already satisfied: requests in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: gradio in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (5.10.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from spacy) (2.10.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from spacy) (3.1.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from spacy) (75.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: click in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from pdfplumber) (11.1.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from pdfplumber) (4.30.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (44.0.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from transformers) (0.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from requests) (2024.12.14)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from gradio) (4.8.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from gradio) (0.115.6)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from gradio) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.5.3 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from gradio) (1.5.3)\n",
      "Requirement already satisfied: httpx>=0.24.1 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from gradio) (0.27.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from gradio) (3.10.13)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: pydub in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.2.2 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from gradio) (0.8.6)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from gradio) (0.41.3)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from gradio) (0.13.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from gradio) (0.34.0)\n",
      "Requirement already satisfied: websockets<15.0,>=10.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from gradio-client==1.5.3->gradio) (14.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     -------------- ------------------------- 4.7/12.8 MB 25.9 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.6/12.8 MB 41.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.6/12.8 MB 41.5 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 18.7 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy nltk pdfplumber docx2txt scikit-learn torch transformers faiss-cpu requests gradio\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37c070be-d8ed-46db-90bb-0c2eb3e003c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pdfplumber\n",
    "import docx2txt\n",
    "import requests\n",
    "import spacy\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74b6157e-b515-4ee6-8df7-debb7d46d4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\GAD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\GAD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\GAD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dc90e65-0f22-4884-89d5-547e5a78c58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    " \n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\"\n",
    " \n",
    "# Initialize the OpenAI client for Ollama integration\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "828004af-fac7-4e8a-8ac6-f17349ed7099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define job roles and required skills\n",
    "JOB_ROLES = {\n",
    "    \"Data Scientist\": {\"Python\", \"SQL\", \"Machine Learning\", \"Deep Learning\", \"NLP\", \"Statistics\", \"Pandas\", \"Scikit-Learn\"},\n",
    "    \"Software Engineer\": {\"Python\", \"Java\", \"C++\", \"Git\", \"OOP\", \"Algorithms\"},\n",
    "    \"Cloud Engineer\": {\"AWS\", \"Azure\", \"Docker\", \"Kubernetes\", \"Terraform\", \"Networking\"},\n",
    "    \"Cybersecurity Analyst\": {\"Cybersecurity\", \"Ethical Hacking\", \"Network Security\", \"Penetration Testing\"},\n",
    "    \"AI Engineer\": {\"Python\", \"TensorFlow\", \"PyTorch\", \"Machine Learning\", \"Deep Learning\", \"AI\"}\n",
    "}\n",
    "\n",
    "# Predefined common skills\n",
    "COMMON_SKILLS = {\n",
    "    \"Python\", \"Java\", \"C++\", \"SQL\", \"Machine Learning\", \"Deep Learning\", \"NLP\", \"Pandas\", \"Scikit-Learn\",\n",
    "    \"TensorFlow\", \"PyTorch\", \"Data Analysis\", \"Cybersecurity\", \"Ethical Hacking\", \"AWS\", \"Azure\", \"Docker\",\n",
    "    \"Kubernetes\", \"Flask\", \"Django\", \"Linux\", \"JavaScript\", \"React\", \"Node.js\", \"Computer Vision\", \"Statistics\",\n",
    "    \"Mathematics\", \"Tableau\", \"Power BI\", \"Time Management\", \"Problem Solving\", \"Communication\", \"Teamwork\"\n",
    "}\n",
    "\n",
    "# Learning recommendations for missing skills\n",
    "LEARNING_RESOURCES = {\n",
    "    \"Python\": \"https://www.udemy.com/course/python-for-data-science-and-machine-learning-bootcamp/\",\n",
    "    \"SQL\": \"https://www.coursera.org/learn/sql-for-data-science\",\n",
    "    \"Machine Learning\": \"https://www.coursera.org/learn/machine-learning\",\n",
    "    \"Deep Learning\": \"https://www.udemy.com/course/deep-learning-a-z/\",\n",
    "    \"NLP\": \"https://www.udemy.com/course/nlp-natural-language-processing-with-python/\",\n",
    "    \"Statistics\": \"https://www.khanacademy.org/math/statistics-probability\",\n",
    "    \"AWS\": \"https://www.udemy.com/course/aws-certified-solutions-architect-associate/\",\n",
    "    \"Cybersecurity\": \"https://www.udemy.com/course/the-complete-cyber-security-course-hacker-exposed/\",\n",
    "}\n",
    "\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text if text else \"Error extracting text from PDF\"\n",
    "\n",
    "def extract_skills(text):\n",
    "    \"\"\"Extract skills from resume using predefined list and NLP.\"\"\"\n",
    "    extracted_skills = set()\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Match predefined skills\n",
    "    for skill in COMMON_SKILLS:\n",
    "        if skill.lower() in text_lower:\n",
    "            extracted_skills.add(skill)\n",
    "\n",
    "    # Use spaCy for Named Entity Recognition (NER)\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in [\"ORG\", \"PERSON\", \"GPE\", \"FACILITY\", \"EVENT\"]:  # Avoid extracting non-skills\n",
    "            continue\n",
    "        if ent.text in COMMON_SKILLS:  # Extract only valid skills\n",
    "            extracted_skills.add(ent.text)\n",
    "\n",
    "    return list(extracted_skills)\n",
    "\n",
    "def extract_summary(text):\n",
    "    \"\"\"Extract a concise summary from the resume using NLP.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    for sent in doc.sents:\n",
    "        if len(sent.text.split()) > 5:  # Ensure it's a meaningful sentence\n",
    "            return sent.text\n",
    "    return \"Summary not found.\"\n",
    "\n",
    "def calculate_match_score(resume_skills, job_role):\n",
    "    \"\"\"Calculate match score and identify matched/missing skills.\"\"\"\n",
    "    required_skills = JOB_ROLES.get(job_role, set())\n",
    "    if not required_skills:\n",
    "        return 0.0, set(), set(), []\n",
    "\n",
    "    matched_skills = set(resume_skills) & required_skills\n",
    "    missing_skills = required_skills - matched_skills\n",
    "    match_percentage = (len(matched_skills) / len(required_skills)) * 100\n",
    "\n",
    "    # Generate learning recommendations\n",
    "    learning_links = [f\"{skill}: {LEARNING_RESOURCES.get(skill, 'No course available')}\" for skill in missing_skills]\n",
    "\n",
    "    return round(match_percentage, 2), matched_skills, missing_skills, learning_links\n",
    "\n",
    "def process_resume(file, job_role):\n",
    "    \"\"\"Process resume, extract skills, summary, and calculate match score.\"\"\"\n",
    "    resume_text = extract_text_from_pdf(file.name)\n",
    "    extracted_skills = extract_skills(resume_text)\n",
    "    summary = extract_summary(resume_text)\n",
    "    match_score, matched_skills, missing_skills, learning_links = calculate_match_score(extracted_skills, job_role)\n",
    "\n",
    "    return f\" **Match Score:** {match_score}%\\n\\n\" \\\n",
    "           f\" **Resume Summary:** {summary}\\n\\n\" \\\n",
    "           f\" **Extracted Skills:** {', '.join(extracted_skills) if extracted_skills else 'None'}\\n\\n\" \\\n",
    "           f\" **Matched Skills:** {', '.join(matched_skills) if matched_skills else 'None'}\\n\\n\" \\\n",
    "           f\" **Missing Skills:** {', '.join(missing_skills) if missing_skills else 'None'}\\n\\n\" \\\n",
    "           f\" **Learning Recommendations:**\\n{chr(10).join(learning_links) if learning_links else 'No recommendations'}\"\n",
    "\n",
    "# Gradio UI\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## 📄 Resume Skill Matcher & Learning Recommendations\")\n",
    "    gr.Markdown(\"Upload your resume and select a job role to check your match score and get learning recommendations.\")\n",
    "\n",
    "    file_input = gr.File(label=\"Upload Resume (PDF)\")\n",
    "    job_dropdown = gr.Dropdown(choices=list(JOB_ROLES.keys()), label=\"Select Job Role\")\n",
    "    output_text = gr.Textbox(label=\"Result\", interactive=False)\n",
    "\n",
    "    submit_button = gr.Button(\"Check Match Score\")\n",
    "    submit_button.click(fn=process_resume, inputs=[file_input, job_dropdown], outputs=output_text)\n",
    "\n",
    "# Run the Gradio app\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9d8a11-7c54-4b32-a720-20135bea4492",
   "metadata": {},
   "source": [
    "### VERSION 2 BY USING LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b7145f0-a3aa-42ed-ad2e-0dee90051ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b996cdb-1e4c-406f-9418-6e0a9e6b0937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7878\n",
      "* Running on public URL: https://880fd94b9621361725.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://880fd94b9621361725.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "JOB_ROLES = {\n",
    "    \"Data Scientist\": {\"Python\", \"SQL\", \"Machine Learning\", \"Deep Learning\", \"NLP\", \"Statistics\", \"Pandas\", \"Scikit-Learn\"},\n",
    "    \"Software Engineer\": {\"Python\", \"Java\", \"C++\", \"Git\", \"OOP\", \"Algorithms\"},\n",
    "    \"Cloud Engineer\": {\"AWS\", \"Azure\", \"Docker\", \"Kubernetes\", \"Terraform\", \"Networking\"},\n",
    "    \"Cybersecurity Analyst\": {\"Cybersecurity\", \"Ethical Hacking\", \"Network Security\", \"Penetration Testing\"},\n",
    "    \"AI Engineer\": {\"Python\", \"TensorFlow\", \"PyTorch\", \"Machine Learning\", \"Deep Learning\", \"AI\"}\n",
    "}\n",
    "\n",
    "# Learning resources for missing skills\n",
    "LEARNING_RESOURCES = {\n",
    "    \"Python\": \"https://www.udemy.com/course/python-for-data-science-and-machine-learning-bootcamp/\",\n",
    "    \"SQL\": \"https://www.coursera.org/learn/sql-for-data-science\",\n",
    "    \"Machine Learning\": \"https://www.coursera.org/learn/machine-learning\",\n",
    "    \"Deep Learning\": \"https://www.udemy.com/course/deep-learning-a-z/\",\n",
    "    \"NLP\": \"https://www.udemy.com/course/nlp-natural-language-processing-with-python/\",\n",
    "    \"Statistics\": \"https://www.khanacademy.org/math/statistics-probability\",\n",
    "    \"AWS\": \"https://www.udemy.com/course/aws-certified-solutions-architect-associate/\",\n",
    "    \"Cybersecurity\": \"https://www.udemy.com/course/the-complete-cyber-security-course-hacker-exposed/\",\n",
    "}\n",
    "\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            extracted = page.extract_text()\n",
    "            if extracted:\n",
    "                text += extracted + \"\\n\"\n",
    "    return text if text else \"Error extracting text from PDF\"\n",
    "\n",
    "def extract_summary_with_llama(text):\n",
    "    \"\"\"Generate a professional summary using LLaMA 3.2.\"\"\"\n",
    "    response = ollama_via_openai.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Summarize this resume in a professional tone: {text}\"}]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def extract_skills_fast(text):\n",
    "    \"\"\"Fast skill extraction using regex-based keyword matching.\"\"\"\n",
    "    skills = set()\n",
    "    lowercase_text = text.lower()\n",
    "    \n",
    "    for job, required_skills in JOB_ROLES.items():\n",
    "        for skill in required_skills:\n",
    "            if re.search(rf\"\\b{skill.lower()}\\b\", lowercase_text):\n",
    "                skills.add(skill)\n",
    "\n",
    "    return list(skills)\n",
    "\n",
    "def calculate_match_score(resume_skills, job_role):\n",
    "    \"\"\"Calculate match score and identify matched/missing skills.\"\"\"\n",
    "    required_skills = JOB_ROLES.get(job_role, set())\n",
    "    if not required_skills:\n",
    "        return 0.0, set(), set(), []\n",
    "\n",
    "    matched_skills = set(resume_skills) & required_skills\n",
    "    missing_skills = required_skills - matched_skills\n",
    "    match_percentage = (len(matched_skills) / len(required_skills)) * 100 if required_skills else 0\n",
    "\n",
    "    # Generate learning recommendations\n",
    "    learning_links = [f\"{skill}: {LEARNING_RESOURCES.get(skill, 'No course available')}\" for skill in missing_skills]\n",
    "\n",
    "    return round(match_percentage, 2), matched_skills, missing_skills, learning_links\n",
    "\n",
    "def process_resume(file, job_role):\n",
    "    \"\"\"Process resume, extract skills (fast), summary (LLaMA), and calculate match score.\"\"\"\n",
    "    resume_text = extract_text_from_pdf(file.name)\n",
    "\n",
    "    # Use LLaMA only for summary\n",
    "    summary = extract_summary_with_llama(resume_text)\n",
    "\n",
    "    # Use fast skill extraction (regex)\n",
    "    extracted_skills = extract_skills_fast(resume_text)\n",
    "\n",
    "    match_score, matched_skills, missing_skills, learning_links = calculate_match_score(extracted_skills, job_role)\n",
    "\n",
    "    return f\" **Match Score:** {match_score}%\\n\\n\" \\\n",
    "           f\" **Resume Summary:** {summary}\\n\\n\" \\\n",
    "           f\" **Extracted Skills:** {', '.join(extracted_skills) if extracted_skills else 'None'}\\n\\n\" \\\n",
    "           f\" **Matched Skills:** {', '.join(matched_skills) if matched_skills else 'None'}\\n\\n\" \\\n",
    "           f\" **Missing Skills:** {', '.join(missing_skills) if missing_skills else 'None'}\\n\\n\" \\\n",
    "           f\" **Learning Recommendations:**\\n{chr(10).join(learning_links) if learning_links else 'No recommendations'}\"\n",
    "\n",
    "# Gradio UI\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## 📄 Resume Skill Matcher & Learning Recommendations (Optimized)\")\n",
    "    gr.Markdown(\"Upload your resume and select a job role to check your match score and get learning recommendations.\")\n",
    "\n",
    "    file_input = gr.File(label=\"Upload Resume (PDF)\")\n",
    "    job_dropdown = gr.Dropdown(choices=list(JOB_ROLES.keys()), label=\"Select Job Role\")\n",
    "    output_text = gr.Textbox(label=\"Result\", interactive=False)\n",
    "\n",
    "    submit_button = gr.Button(\"Check Match Score\")\n",
    "    submit_button.click(fn=process_resume, inputs=[file_input, job_dropdown], outputs=output_text)\n",
    "\n",
    "# Run the Gradio app\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8106e2d5-6f85-4372-a178-44e0f14fef45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
