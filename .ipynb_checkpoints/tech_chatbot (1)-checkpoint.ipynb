{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d006b2ea-9dfe-49c7-88a9-a5a0775185fd",
   "metadata": {},
   "source": [
    "# Additional End of week Exercise - week 2\n",
    "\n",
    "Now use everything you've learned from Week 2 to build a full prototype for the technical question/answerer you built in Week 1 Exercise.\n",
    "\n",
    "This should include a Gradio UI, streaming, use of the system prompt to add expertise, and the ability to switch between models. Bonus points if you can demonstrate use of a tool!\n",
    "\n",
    "If you feel bold, see if you can add audio input so you can talk to it, and have it respond with audio. ChatGPT or Claude can help you, or email me if you have questions.\n",
    "\n",
    "I will publish a full solution here soon - unless someone beats me to it...\n",
    "\n",
    "There are so many commercial applications for this, from a language tutor, to a company onboarding solution, to a companion AI to a course (like this one!) I can't wait to see your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a07e7793-b8f5-44f4-aded-5562f633271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dd8df97-5549-4ddf-a2f8-faf5b8d08eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba0f3809-ebdc-45b0-87bb-63f6f4e26ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26d4bead-6aa8-4ce5-8c45-5547a0897f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â � \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling dde5aa3fc5ff... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 2.0 GB                         \n",
      "pulling 966de95ca8a6... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 1.4 KB                         \n",
      "pulling fcc5a6bec9da... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 7.7 KB                         \n",
      "pulling a70ff7e570d9... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 6.0 KB                         \n",
      "pulling 56bb8bd477a5... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–�   96 B                         \n",
      "pulling 34bb5ab01051... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–�  561 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76f9bc82-edcd-443e-b62f-1c0633273fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cda250cc-b56a-4a34-b4c0-5569f6d30c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant for answering technical questions.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93ae3f7c-7399-47ce-878a-bae11ff2b999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â � \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling dde5aa3fc5ff... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 2.0 GB                         \n",
      "pulling 966de95ca8a6... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 1.4 KB                         \n",
      "pulling fcc5a6bec9da... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 7.7 KB                         \n",
      "pulling a70ff7e570d9... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 6.0 KB                         \n",
      "pulling 56bb8bd477a5... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–�   96 B                         \n",
      "pulling 34bb5ab01051... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–�  561 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n",
      "C:\\Users\\GAD\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\interface.py:403: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7874\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7874/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\"\n",
    "\n",
    "# Make sure to pull the necessary model\n",
    "!ollama pull llama3.2\n",
    "\n",
    "# System message for the assistant (adds expertise)\n",
    "system_message = \"You are a highly knowledgeable technical expert. Please provide accurate, clear, and concise answers to technical questions.\"\n",
    "\n",
    "# Initialize the OpenAI client for Ollama integration\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "# This function will handle the user's query and ensure lists are handled properly\n",
    "def get_technical_explanation(question, history):\n",
    "    \"\"\"\n",
    "    Takes a technical question and returns an explanation from the model.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The technical question to ask the model.\n",
    "        history (list): Conversation history\n",
    "        \n",
    "    Returns:\n",
    "        str: The model's response containing the explanation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Make sure history is always a list (if None, initialize as an empty list)\n",
    "        history = history or []\n",
    "        \n",
    "        # Structured prompt for the model\n",
    "        prompt = (\n",
    "            \"You are a highly knowledgeable technical expert. \"\n",
    "            \"Provide a clear, detailed, and accurate explanation for the following question:\\n\\n\"\n",
    "            f\"Question: {question}\\n\\n\"\n",
    "            \"Explanation:\"\n",
    "        )\n",
    "        \n",
    "        # Make a request to the model\n",
    "        response = ollama_via_openai.completions.create(\n",
    "            model=MODEL,  # Replace with the appropriate model name\n",
    "            prompt=prompt,\n",
    "            max_tokens=300,  # Adjust as needed\n",
    "            temperature=0.7,  # Adjust creativity level\n",
    "        )\n",
    "        \n",
    "        # Extract the explanation from the response\n",
    "        explanation = response.choices[0].text.strip()\n",
    "\n",
    "        # Return the explanation along with the conversation history\n",
    "        history.append({\"role\": \"assistant\", \"content\": explanation})\n",
    "        return explanation, history\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\", history\n",
    "\n",
    "\n",
    "# Define the Gradio UI and chat function\n",
    "def chat(message, history):\n",
    "    \"\"\"\n",
    "    Handles the user's message and returns the assistant's response.\n",
    "    \n",
    "    Args:\n",
    "        message (str): The user's message.\n",
    "        history (list): Conversation history.\n",
    "        \n",
    "    Returns:\n",
    "        str: The assistant's response.\n",
    "        list: Updated history.\n",
    "    \"\"\"\n",
    "    # Ensure history is a list (if None, initialize as an empty list)\n",
    "    history = history or []\n",
    "    \n",
    "    # Add system message and user query to history\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    # Make a request to the assistant and get the response\n",
    "    response = ollama_via_openai.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    assistant_response = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Update history with the assistant's response\n",
    "    history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "    \n",
    "    return assistant_response, history\n",
    "\n",
    "\n",
    "# Initialize the Gradio interface for chatbot-style interaction\n",
    "gr.Interface(\n",
    "    fn=chat,\n",
    "    inputs=[gr.Textbox(label=\"Ask a Technical Question\", placeholder=\"Type your question here...\", lines=2), gr.State()],\n",
    "    outputs=[gr.Textbox(label=\"Answer\", interactive=False), gr.State()],\n",
    "    live=False,  # Disable real-time submission\n",
    "    title=\"Technical Question Answer Bot\",\n",
    "    description=\"Ask any technical question, and I'll provide an explanation.\",\n",
    "    allow_flagging=\"never\"\n",
    ").launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96b5aad-b945-421f-9791-4d051ae4a4da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47102924-5248-4ac3-99cd-a7278d5c0e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GAD\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\components\\chatbot.py:249: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n",
      "C:\\Users\\GAD\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\interface.py:403: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7883\n",
      "* Running on public URL: https://ca13b1a013ad759097.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://ca13b1a013ad759097.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_technical_explanation(question):\n",
    "    \"\"\"\n",
    "    Takes a technical question and returns an explanation from the model.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The technical question to ask the model.\n",
    "        \n",
    "    Returns:\n",
    "        str: The model's response containing the explanation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Structured prompt for technical explanation\n",
    "        prompt = (\n",
    "            \"You are a highly knowledgeable technical expert. \"\n",
    "            \"Provide a clear, detailed, and accurate explanation for the following question:\\n\\n\"\n",
    "            f\"Question: {question}\\n\\n\"\n",
    "            \"Explanation:\"\n",
    "        )\n",
    "        \n",
    "        # Make a request to the OpenAI API\n",
    "        # Make a request to the model\n",
    "        response = ollama_via_openai.completions.create(\n",
    "            model=MODEL,  # Replace with the appropriate model name\n",
    "            prompt=prompt,\n",
    "            max_tokens=300,  # Adjust as needed\n",
    "            temperature=0.7,  # Adjust creativity level\n",
    "        )\n",
    "        \n",
    "        # Extract and return the explanation from the response\n",
    "        explanation = response.choices[0].text.strip()\n",
    "        return explanation\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "# Function to handle the conversation flow\n",
    "def chat(message, history):\n",
    "    \"\"\"\n",
    "    Handles the user's message and returns the assistant's response.\n",
    "    \n",
    "    Args:\n",
    "        message (str): The user's message.\n",
    "        history (list): The conversation history.\n",
    "        \n",
    "    Returns:\n",
    "        str: The assistant's response.\n",
    "        list: Updated history with the assistant's response.\n",
    "    \"\"\"\n",
    "    if history is None:\n",
    "        history = []  # Initialize history as an empty list if it's None\n",
    "    \n",
    "    # Append the user message to the history\n",
    "    history.append((\"user\", message))\n",
    "    \n",
    "    # Generate the technical explanation for the message\n",
    "    explanation = get_technical_explanation(message)\n",
    "    \n",
    "    # Append the assistant's response to the history\n",
    "    history.append((\"assistant\", explanation))\n",
    "    \n",
    "    # Return the updated history (for display in the chat interface)\n",
    "    return history, history\n",
    "\n",
    "# Create the Gradio interface\n",
    "gr.Interface(\n",
    "    fn=chat,  # Function that handles the conversation\n",
    "    inputs=[gr.Textbox(label=\"Your Message\", placeholder=\"Type your question here...\", lines=2), gr.State()],\n",
    "    outputs=[gr.Chatbot(), gr.State()],\n",
    "    live=False,  # Disable live input (wait for submit)\n",
    "    title=\"Technical Question Answer Bot\",\n",
    "    description=\"Ask me anything technical, and I'll provide detailed explanations.\",\n",
    "    allow_flagging=\"never\"  # Disable flagging\n",
    ").launch(s=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c37807d-e3a6-473c-ad5b-21e77193ac12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
